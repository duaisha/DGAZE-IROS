{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEEPGAZE] head_pose_estimation.py: the dlib library is installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#############################3   Import Packages ##########################################3\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from deepgaze.head_pose_estimation import CnnHeadPoseEstimator\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "    return (x, y, w, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eyes_data(videofile, outputfile):\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"lol.dat\")\n",
    "\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "    left_arr = []; right_arr = []\n",
    "\n",
    "    i = 0\n",
    "    vid = cv2.VideoCapture(videofile)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        if(ret == False):\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        out_face=np.zeros_like(frame)\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            leftEye = np.array(shape[lStart:lEnd])\n",
    "            rightEye = np.array( shape[rStart:rEnd])\n",
    "\n",
    "            rect_x = leftEye[0,0];\n",
    "            rect_y = leftEye[1,1]; \n",
    "            rect_x2 = leftEye[3,0]; \n",
    "            rect_y2 = leftEye[5,1];\n",
    "            \n",
    "            right_rect_x = rightEye[0,0]; \n",
    "            right_rect_y = rightEye[1,1]; \n",
    "            right_rect_x2 = rightEye[3,0]; \n",
    "            right_rect_y2 = rightEye[5,1]\n",
    "\n",
    "            crop_img = frame[ rect_y-15:rect_y2+15, rect_x-15:rect_x2+15]\n",
    "            right_crop_img = frame[ right_rect_y-15:right_rect_y2+15, right_rect_x-15:right_rect_x2+15]\n",
    "\n",
    "\n",
    "        left_arr.append(cv2.resize(crop_img,(60,36)))\n",
    "        right_arr.append(cv2.resize(right_crop_img,(60,36)))\n",
    "        i=i+1\n",
    "\n",
    "    print(i, np.array(left_arr).shape, np.array(right_arr).shape)\n",
    "    \n",
    "    np.save(outputfile +'_left_eye_data.npy', np.array(left_arr))\n",
    "    np.save(outputfile +'_right_eye_data.npy', np.array(right_arr))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose(videofile, outputfile):\n",
    "    \n",
    "    sess = tf.Session() #Launch the graph in a session.\n",
    "    my_head_pose_estimator = CnnHeadPoseEstimator(sess) #Head pose estimation object\n",
    "    my_head_pose_estimator.load_roll_variables(os.path.realpath(\"/ssd_scratch/cvit/isha2/DGM_final2/deepgaze/etc/tensorflow/head_pose/roll/cnn_cccdd_30k.tf\"));\n",
    "    my_head_pose_estimator.load_pitch_variables(os.path.realpath(\"/ssd_scratch/cvit/isha2/DGM_final2/deepgaze/etc/tensorflow/head_pose/pitch/cnn_cccdd_30k.tf\"));\n",
    "    my_head_pose_estimator.load_yaw_variables(os.path.realpath(\"/ssd_scratch/cvit/isha2/DGM_final2/deepgaze/etc/tensorflow/head_pose/yaw/cnn_cccdd_30k.tf\"));\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"lol.dat\")\n",
    "\n",
    "    i = 0\n",
    "    vid = cv2.VideoCapture(videofile)\n",
    "    \n",
    "    arr = []\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        if(ret == False):\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width = 450)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        out_face = np.zeros_like(frame)\n",
    "        rects = detector(gray, 0)\n",
    "        crop_img = out_face\n",
    "        \n",
    "        for rect in rects:\n",
    "\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            #print(shape.shape)\n",
    "\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            remapped_shape = np.zeros_like(shape)\n",
    "            feature_mask = np.zeros((frame.shape[0], frame.shape[1]))\n",
    "            #print(feature_mask.shape)\n",
    "            remapped_shape = cv2.convexHull(shape)\n",
    "            cv2.drawContours(frame, [remapped_shape], -1, (0, 255, 0), 1)\n",
    "\n",
    "            cv2.fillConvexPoly(feature_mask, remapped_shape[0:27], 1)\n",
    "            feature_mask = feature_mask.astype(np.bool)\n",
    "            out_face[feature_mask] = frame[feature_mask]\n",
    "\n",
    "            x = min(shape[:,0])-20\n",
    "            y = min(shape[:,1])-20\n",
    "            w = max(shape[:,0]) -min(shape[:,0]) +50\n",
    "            h = max(shape[:,1]) -min(shape[:,1]) +50\n",
    "            crop_img = out_face[y:y+h, x:x+w]\n",
    "#             plt.imshow(crop_img)\n",
    "#             plt.show()\n",
    "\n",
    "        if(np.sum(crop_img) == 0):\n",
    "            print('face not detected')\n",
    "            temp =[]\n",
    "            temp.append(i)\n",
    "            temp.append(0)\n",
    "            temp.append(0)\n",
    "            temp.append(0)\n",
    "            arr.append(temp)\n",
    "            continue;\n",
    "        else:\n",
    "            image = crop_img\n",
    "            image = cv2.resize(image,(200,200))\n",
    "\n",
    "            temp =[]\n",
    "            roll = my_head_pose_estimator.return_roll(image)  # Evaluate the roll angle using a CNN\n",
    "            pitch = my_head_pose_estimator.return_pitch(image)  # Evaluate the pitch angle using a CNN\n",
    "            yaw = my_head_pose_estimator.return_yaw(image)  # Evaluate the yaw angle using a CNN\n",
    "\n",
    "            temp.append(i)\n",
    "            temp.append(roll[0,0,0])\n",
    "            temp.append(pitch[0,0,0])\n",
    "            temp.append(yaw[0,0,0])\n",
    "                \n",
    "            arr.append(temp)\n",
    "        i += 1\n",
    "\n",
    "    print(i, np.array(arr).shape)\n",
    "    # np.save(outputfile +'_headpose.npy', np.array(arr))\n",
    "    return np.array(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pupil_location(videofile, outputfile):\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"lol.dat\")\n",
    "\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "    left_arr = []; right_arr = []\n",
    "\n",
    "    i = 0; arr = []\n",
    "    vid = cv2.VideoCapture(videofile)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        if(ret == False):\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        out_face=np.zeros_like(frame)\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            tl_x ,tl_y = shape[0,0], shape[19,1]\n",
    "            br_x ,br_y = shape[16,0], shape[8,1]\n",
    "\n",
    "            area = (br_y - tl_y)*(br_x - tl_x)\n",
    "\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            cv2.rectangle(frame, (x-5, y-5), (br_x+5, br_y+5), (0, 255, 0), 2)\n",
    "\n",
    "            nose_x, nose_y = shape[33,0], shape[33,1]\n",
    "            \n",
    "            leftEye = np.array(shape[lStart:lEnd])\n",
    "            rightEye = np.array( shape[rStart:rEnd])\n",
    "            #print(leftEye[0,0],(leftEye).shape)\n",
    "\n",
    "            pt1_x=(np.sum(leftEye[1,0] + leftEye[2,0]))/2.0\n",
    "            pt1_y=(np.sum(leftEye[1,1] + leftEye[2,1]))/2.0\n",
    "            pt2_x=(np.sum(leftEye[5,0] + leftEye[4,0]))/2.0\n",
    "            pt2_y=(np.sum(leftEye[5,1] + leftEye[4,1]))/2.0\n",
    "            pt_x=int(np.sum(pt1_x + pt2_x)/2.0)\n",
    "            pt_y=int(np.sum(pt1_y + pt2_y)/2.0)\n",
    "\n",
    "            pt1r_x=(np.sum(rightEye[1,0] + rightEye[2,0]))/2.0\n",
    "            pt1r_y=(np.sum(rightEye[1,1] + rightEye[2,1]))/2.0\n",
    "            pt2r_x=(np.sum(rightEye[5,0] + rightEye[4,0]))/2.0\n",
    "            pt2r_y=(np.sum(rightEye[5,1] + rightEye[4,1]))/2.0\n",
    "            ptr_x=int(np.sum(pt1r_x + pt2r_x)/2.0)\n",
    "            ptr_y=int(np.sum(pt1r_y + pt2r_y)/2.0)\n",
    "            \n",
    "            cv2.circle(frame,(int(pt_x), int(pt_y)),3,(255,255,255),4)\n",
    "            cv2.circle(frame,(int(ptr_x), int(ptr_y)),3,(255,255,255),4)\n",
    "\n",
    "#             plt.imshow(frame)\n",
    "#             plt.show()\n",
    "            \n",
    "        \n",
    "        temp =[]\n",
    "        \n",
    "        temp.append(i)\n",
    "        temp.append(pt_x)\n",
    "        temp.append(pt_y)\n",
    "        temp.append(ptr_x)\n",
    "        temp.append(ptr_y)\n",
    "        temp.append(area)\n",
    "        temp.append(nose_x)\n",
    "        temp.append(nose_y)\n",
    "        \n",
    "        arr.append(temp)\n",
    "        i += 1\n",
    "        \n",
    "    print(i, np.array(arr).shape)\n",
    "    #np.save(outputfile +'_pupil.npy', np.array(arr))\n",
    "    return np.array(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8ff0de225718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/driver_view_cropped/sample_\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".avi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdest_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"sample\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0meyes_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mheadpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpupil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pupil_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2c28ef43006c>\u001b[0m in \u001b[0;36meyes_data\u001b[0;34m(videofile, outputfile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meyes_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lol.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ur = 14\n",
    "\n",
    "directory = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(ur)+\"/explicit_face_features_game/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(0,112):\n",
    "    print(ur, i)\n",
    "    video_name = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(ur)+\"/driver_view_cropped/sample_\" +str(i+1)+\".avi\"\n",
    "    dest_folder = directory + \"sample\" +str(i+1)\n",
    "    eyes_data(video_name, dest_folder)\n",
    "    headpose = get_pose(video_name, dest_folder)\n",
    "    pupil = get_pupil_location(video_name, dest_folder)\n",
    "    \n",
    "    if(headpose.shape[0]==0 or pupil.shape[0]==0):\n",
    "        continue\n",
    "    #print(headpose.shape, pupil.shape)\n",
    "    headpose_pupil = np.concatenate((headpose, pupil[:,1:]), axis =1)\n",
    "    print(headpose_pupil.shape)\n",
    "    np.save(dest_folder +'_headpose_pupil.npy', np.array(headpose_pupil))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
