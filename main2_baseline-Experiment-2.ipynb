{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation, GRU, Flatten,Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import imutils\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(data):\n",
    "    arr_row = []\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        pt1 = row[1] + (row[3] - row[1])/2\n",
    "        pt2 = row[2] + (row[4] - row[2])/2\n",
    "        temp =[int(pt1), int(pt2)]\n",
    "        arr_row.append(temp)\n",
    "        \n",
    "    return np.array(arr_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(folder,n):\n",
    "    arr_X_lefteye = []; \n",
    "    arr_X_righteye = []; \n",
    "    arr_X_face_features = []\n",
    "\n",
    "    arr_Y =[]\n",
    "    \n",
    "    for i in tqdm(range(112)):\n",
    "\n",
    "        \n",
    "        filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "        filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "        filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        if(os.path.exists(\"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/\") and os.path.exists(filenameY)\n",
    "          and os.path.exists(filenameX +\"_left_eye_data.npy\") and os.path.exists(filenameX +\"_right_eye_data.npy\")\n",
    "          and os.path.exists(filenameX +\"_headpose_pupil.npy\")):\n",
    "            \n",
    "            x_lefteye = np.load(filenameX +\"_left_eye_data.npy\")\n",
    "            x_righteye = np.load(filenameX +\"_right_eye_data.npy\")\n",
    "            x_face_features = np.load(filenameX +\"_headpose_pupil.npy\")\n",
    "            x_face_points = np.load(filenameX_face_points +\"_face_points.npy\")\n",
    "            #print(i)\n",
    "            y = np.load(filenameY)\n",
    "\n",
    "            if(y.shape[0]>=n):\n",
    "                \n",
    "                arr_X_lefteye.append(x_lefteye[:n])\n",
    "                arr_X_righteye.append(x_righteye[:n])\n",
    "                arr_X_face_features.append(np.concatenate((x_face_features[:n], x_face_points[:n]),axis =1))\n",
    "      \n",
    "                if(i>=9):\n",
    "                    arr_Y.append(np.hstack((get_value(y[:n,:]),y[:n,1:])))\n",
    "                else: \n",
    "                    arr_Y.append(np.hstack((y[:n,:],np.ones((n,4))*-1)))\n",
    "\n",
    "   # print(np.array(arr_X_lefteye).shape, np.array(arr_X_righteye).shape, np.array(arr_X_face_features).shape, np.array(arr_Y).shape)\n",
    "    return np.array(arr_X_lefteye), np.array(arr_X_righteye), np.array(arr_X_face_features), np.array(arr_Y)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data2(users):\n",
    "#     left_eye_data = []; right_eye_data = []; face_features_data = []; Ydata = []\n",
    "    \n",
    "#     for i in range(len(users)):\n",
    "\n",
    "#         left_eye, right_eye, face_features, Yground_truth = get_features(users[i], 50)\n",
    "#         print(left_eye.shape, right_eye.shape, face_features.shape, Yground_truth.shape)\n",
    "\n",
    "#         left_eye_data.append(left_eye)\n",
    "#         right_eye_data.append(right_eye)\n",
    "#         face_features_data.append(face_features)\n",
    "#         Ydata.append(Yground_truth)\n",
    "\n",
    "#     return left_eye_data, right_eye_data, face_features_data, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(users):\n",
    "    \n",
    "    for i in tqdm(range(len(users))):\n",
    "\n",
    "        left_eye, right_eye, face_features, Yground_truth = get_features(users[i], 50)\n",
    "\n",
    "        if(i == 0):\n",
    "            left_eye_data = left_eye\n",
    "            right_eye_data = right_eye\n",
    "            face_features_data = face_features\n",
    "            Ydata = Yground_truth\n",
    "\n",
    "        else:\n",
    "            left_eye_data = np.concatenate((left_eye_data, left_eye), axis = 0)\n",
    "            right_eye_data = np.concatenate((right_eye_data, right_eye), axis = 0)\n",
    "            face_features_data = np.concatenate((face_features_data, face_features), axis = 0)\n",
    "            Ydata = np.concatenate((Ydata, Yground_truth), axis = 0)\n",
    "\n",
    "    return left_eye_data, right_eye_data, face_features_data, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 38%|███▊      | 43/112 [00:00<00:00, 425.69it/s]\u001b[A\n",
      " 79%|███████▊  | 88/112 [00:00<00:00, 431.13it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:05,  3.22it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 51%|█████     | 57/112 [00:00<00:00, 568.02it/s]\u001b[A\n",
      " 97%|█████████▋| 109/112 [00:00<00:00, 552.02it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:05,  3.21it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 58/112 [00:00<00:00, 566.45it/s]\u001b[A\n",
      " 94%|█████████▍| 105/112 [00:00<00:00, 530.15it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:05,  3.10it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 58/112 [00:00<00:00, 572.37it/s]\u001b[A\n",
      " 98%|█████████▊| 110/112 [00:00<00:00, 552.01it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:01<00:05,  2.82it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 58/112 [00:00<00:00, 578.19it/s]\u001b[A\n",
      " 99%|█████████▉| 111/112 [00:00<00:00, 562.18it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:01<00:05,  2.61it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 54%|█████▍    | 61/112 [00:00<00:00, 602.73it/s]\u001b[A\n",
      "100%|██████████| 112/112 [00:00<00:00, 570.55it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:02<00:05,  2.39it/s]\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 55%|█████▌    | 62/112 [00:00<00:00, 614.54it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:02<00:05,  2.22it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 54%|█████▎    | 60/112 [00:00<00:00, 596.36it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:03<00:05,  2.09it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 53%|█████▎    | 59/112 [00:00<00:00, 585.88it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:04<00:05,  1.94it/s]t/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 53%|█████▎    | 59/112 [00:00<00:00, 583.64it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:04<00:05,  1.81it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 53%|█████▎    | 59/112 [00:00<00:00, 586.02it/s]\u001b[A\n",
      "100%|██████████| 112/112 [00:00<00:00, 567.23it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:05<00:05,  1.68it/s]\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 51%|█████     | 57/112 [00:00<00:00, 563.62it/s]\u001b[A\n",
      " 98%|█████████▊| 110/112 [00:00<00:00, 552.54it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:06<00:05,  1.55it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 53%|█████▎    | 59/112 [00:00<00:00, 586.87it/s]\u001b[A\n",
      " 98%|█████████▊| 110/112 [00:00<00:00, 560.83it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:06<00:04,  1.45it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 58/112 [00:00<00:00, 579.25it/s]\u001b[A\n",
      " 99%|█████████▉| 111/112 [00:00<00:00, 563.19it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:07<00:04,  1.36it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 53%|█████▎    | 59/112 [00:00<00:00, 583.36it/s]\u001b[A\n",
      " 99%|█████████▉| 111/112 [00:00<00:00, 560.30it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:08<00:03,  1.28it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 58/112 [00:00<00:00, 579.61it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:09<00:03,  1.22it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 55%|█████▌    | 62/112 [00:00<00:00, 619.18it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:10<00:02,  1.18it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 54%|█████▎    | 60/112 [00:00<00:00, 592.90it/s]\u001b[A\n",
      " 98%|█████████▊| 110/112 [00:00<00:00, 556.64it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:11<00:01,  1.11it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 88%|████████▊ | 99/112 [00:00<00:00, 986.53it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:12<00:00,  1.10it/s]/s]\u001b[A\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      " 97%|█████████▋| 109/112 [00:00<00:00, 1073.11it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.09it/s]/s] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2013, 50, 36, 60, 3) (2013, 50, 36, 60, 3) (2013, 50, 15) (2013, 50, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "left_eye_data, right_eye_data, face_features_data,Ydata = get_data(users)\n",
    "print(left_eye_data.shape, right_eye_data.shape, face_features_data.shape,Ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1054, 587, 886, 456, 1222, 719], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ydata[10,25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert video data into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100650, 36, 60, 3) (100650, 36, 60, 3) (100650, 15) (100650, 6)\n"
     ]
    }
   ],
   "source": [
    "left_eye_data2 = left_eye_data.reshape((left_eye_data.shape[0]*left_eye_data.shape[1], \n",
    "                                         left_eye_data.shape[2],left_eye_data.shape[3],\n",
    "                                         left_eye_data.shape[4]))\n",
    "\n",
    "right_eye_data2 = right_eye_data.reshape((right_eye_data.shape[0]*right_eye_data.shape[1], \n",
    "                                         right_eye_data.shape[2],right_eye_data.shape[3],\n",
    "                                         right_eye_data.shape[4]))\n",
    "\n",
    "face_features_data2 = face_features_data.reshape((face_features_data.shape[0]*face_features_data.shape[1], \n",
    "                                         face_features_data.shape[2]))\n",
    "\n",
    "Ydata2 = Ydata.reshape((Ydata.shape[0]*Ydata.shape[1], \n",
    "                                         Ydata.shape[2]))\n",
    "\n",
    "print(left_eye_data2.shape, right_eye_data2.shape, face_features_data2.shape, Ydata2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into Train and Val\n",
    "\n",
    "1.  Train = 80% of total dataset \n",
    "    - For images\n",
    "       1. left_eye_data2\n",
    "       2. right_eye_data2\n",
    "       3. face_features_data2\n",
    "       4. Ydata2\n",
    "       \n",
    "    - For videos\n",
    "    \n",
    "2.  val = 20% of the dataset\n",
    "3. Test = left_eye10, right_eye10, face_features10, Y10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90585 5032 5033\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "idx = np.arange(0,len(Ydata2))\n",
    "shuffle(idx)\n",
    "\n",
    "train_len = int((90 * len(Ydata2))/100)\n",
    "\n",
    "train_idx = idx[:train_len]\n",
    "#val_idx = idx[train_len:len(Ydata2)]\n",
    "val_idx = idx[train_len:train_len+int((len(Ydata2)-train_len)/2)]\n",
    "test_idx = idx[train_len+int((len(Ydata2)-train_len)/2):len(Ydata2)]\n",
    "\n",
    "print(len(train_idx),len(val_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left_eye_data = left_eye_data2[train_idx]\n",
    "train_right_eye_data = right_eye_data2[train_idx]\n",
    "train_face_features_data = face_features_data2[train_idx] \n",
    "train_Ydata = Ydata2[train_idx]\n",
    "\n",
    "val_left_eye_data = left_eye_data2[val_idx]\n",
    "val_right_eye_data = right_eye_data2[val_idx]\n",
    "val_face_features_data = face_features_data2[val_idx] \n",
    "val_Ydata = Ydata2[val_idx]\n",
    "\n",
    "test_left_eye_data = left_eye_data2[test_idx]\n",
    "test_right_eye_data = right_eye_data2[test_idx]\n",
    "test_face_features_data = face_features_data2[test_idx] \n",
    "test_Ydata = Ydata2[test_idx]\n",
    "\n",
    "\n",
    "# test_left_eye_data = left_eye10.reshape((left_eye10.shape[0]*left_eye10.shape[1], \n",
    "#                                          left_eye10.shape[2],left_eye10.shape[3],\n",
    "#                                          left_eye10.shape[4]))  \n",
    "\n",
    "# test_right_eye_data = right_eye10.reshape((right_eye10.shape[0]*right_eye10.shape[1], \n",
    "#                                          right_eye10.shape[2],right_eye10.shape[3],\n",
    "#                                          right_eye10.shape[4]))  \n",
    "\n",
    "# test_face_features_data = face_features10.reshape((face_features10.shape[0]*face_features10.shape[1], \n",
    "#                                          face_features10.shape[2]))\n",
    "\n",
    "# test_Ydata = Y10.reshape((Y10.shape[0]*Y10.shape[1], \n",
    "#                                          Y10.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90585, 36, 60, 3) (90585, 36, 60, 3) (90585, 15) (90585, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_left_eye_data.shape, train_right_eye_data.shape, train_face_features_data.shape, train_Ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.80000000e+01 -1.71512306e+00  6.39218712e+00  1.78142910e+01\n",
      "  3.21000000e+02  1.98000000e+02  1.96000000e+02  1.99000000e+02\n",
      "  6.79250000e+04  2.44000000e+02  2.97000000e+02  3.55000000e+02\n",
      "  8.76000000e+02  1.05000000e+02  6.26000000e+02]\n",
      "[ -1.71512306   6.39218712  17.814291   321.         198.\n",
      " 196.         199.         244.         297.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_face_features_data[0])\n",
    "### 0. Frame no\n",
    "### 1,2,3. Headpose(1,2,3)\n",
    "### 4,5 Left eye location\n",
    "### 6,7 Right Eye Location\n",
    "### 8 Face Area\n",
    "### 9,10 Nose location\n",
    "\n",
    "print(np.concatenate((train_face_features_data[0,1:8], train_face_features_data[0,9:11])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5920ee0bdda2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(20, kernel_size=(3, 3),activation='relu',input_shape=(36,60,3)))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(50, (3, 3), activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Flatten())\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, activation ='relu', input_dim=(13)))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Merge([model1, model2], mode = 'concat'))\n",
    "model3.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model3.add(Dense(6))\n",
    "print(model3.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.regularizers import l2\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(20, kernel_size=(3, 3),activation='relu',input_shape=(36,60,3)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #model.add(Conv2D(50, (3, 3), activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Conv2D(50, (3, 3), activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Flatten())\n",
    "# inputs = Input(shape=(10,))\n",
    "# #model2= model\n",
    "\n",
    "# model3 = Sequential()\n",
    "# model3.add(Dense(16, activation ='relu', input_dim=(13)))\n",
    "\n",
    "# model2 = Sequential()\n",
    "# model2.add(Merge([model, model3], mode = 'concat'))\n",
    "\n",
    "# model2.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "# #model2.add(Dense(500))\n",
    "\n",
    "# model4 = Sequential()\n",
    "# model5 = Sequential()\n",
    "\n",
    "# model4.add(model2)\n",
    "# model4.add(Dense(256))\n",
    "# model4.add(Dense(2, activation='linear'))\n",
    "\n",
    "# model5.add(model2)\n",
    "# model5.add(Dense(256))\n",
    "# model5.add(Dense(4, activation='linear'))\n",
    "# print(model5.summary())\n",
    "\n",
    "\n",
    "# # out1 = Dense(1,  activation='linear')(x)\n",
    "# # out2 = Dense(1,  activation='linear')(x)\n",
    "# # out3 = Dense(1,  activation='linear')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 14s 157us/step - loss: 196.7135 - val_loss: 179.3282\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 169.9303 - val_loss: 157.9324\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 153.4145 - val_loss: 151.8818\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 13s 149us/step - loss: 143.9956 - val_loss: 144.7342\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 137.2554 - val_loss: 135.7942\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 132.0740 - val_loss: 139.7611\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 127.7805 - val_loss: 129.3746\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 123.6820 - val_loss: 124.9245\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 120.6271 - val_loss: 124.2224\n",
      "Epoch 10/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 117.5854 - val_loss: 122.1418\n",
      "Epoch 11/300\n",
      "90585/90585 [==============================] - 16s 180us/step - loss: 115.4538 - val_loss: 121.3499\n",
      "Epoch 12/300\n",
      "90585/90585 [==============================] - 14s 151us/step - loss: 113.3726 - val_loss: 118.9996\n",
      "Epoch 13/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 111.5383 - val_loss: 121.6771\n",
      "Epoch 14/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 110.2224 - val_loss: 117.8441\n",
      "Epoch 15/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 108.6994 - val_loss: 116.5312\n",
      "Epoch 16/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 107.3539 - val_loss: 115.7271\n",
      "Epoch 17/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 106.1187 - val_loss: 116.1375\n",
      "Epoch 18/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 105.2047 - val_loss: 114.0903\n",
      "Epoch 19/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 104.6483 - val_loss: 113.6200\n",
      "Epoch 20/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 103.6887 - val_loss: 111.9164\n",
      "Epoch 21/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 102.7633 - val_loss: 112.7158\n",
      "Epoch 22/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 101.8131 - val_loss: 109.9669\n",
      "Epoch 23/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 101.0419 - val_loss: 108.7231\n",
      "Epoch 24/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 100.6450 - val_loss: 110.2472\n",
      "Epoch 25/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 99.7878 - val_loss: 110.4878\n",
      "Epoch 26/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 99.1275 - val_loss: 107.8728\n",
      "Epoch 27/300\n",
      "90585/90585 [==============================] - 14s 150us/step - loss: 98.6152 - val_loss: 111.2201\n",
      "Epoch 28/300\n",
      "90585/90585 [==============================] - 14s 149us/step - loss: 98.1961 - val_loss: 111.2867\n",
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='mae', optimizer='adam')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',min_delta = 1, patience =5, verbose =0, mode ='auto')\n",
    "\n",
    "history = model3.fit([train_left_eye_data, np.concatenate((train_face_features_data[:,1:8],train_face_features_data[:,9:15]), axis =1)], train_Ydata[:,:], \n",
    "                     epochs=300, batch_size=40, callbacks=[earlystopping], validation_data=([val_left_eye_data,\n",
    "                     np.concatenate((val_face_features_data[:,1:8],val_face_features_data[:,9:15]),axis = 1)],val_Ydata[:,:]), verbose=1, shuffle= True)\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1172, 537, 1438, 798],\n",
       "       [1032, 540, 1200, 837],\n",
       "       [750, 642, 921, 967],\n",
       "       [708, 586, 978, 705],\n",
       "       [1426, 549, 1563, 710]], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Ydata[:5,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def compute_error(left_eye_data, face_features_data, Ydata):\n",
    "    y_true = Ydata\n",
    "#     y_pred = model2.predict([left_eye_data, face_features_data[:,1:]]).astype(int)\n",
    "#     error = rmse(y_true, y_pred)\n",
    "#     print(error)\n",
    "#     return error\n",
    "\n",
    "#     # load\n",
    "#     json_file = open('model.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     loaded_model = model_from_json(loaded_model_json)\n",
    "#     # load weights into new model\n",
    "#     loaded_model.load_weights(\"model.h5\")\n",
    "#     print(\"Loaded model from disk\")\n",
    "\n",
    "#     # evaluate loaded model on test data\n",
    "#     loaded_model.compile(loss='mae', optimizer='adam')\n",
    "    scores = model2.evaluate([left_eye_data, np.concatenate((face_features_data[:,1:8],face_features_data[:,9:15]),axis = 1)], Ydata)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = compute_error(train_left_eye_data, train_face_features_data, train_Ydata)\n",
    "val_error = compute_error(val_left_eye_data, val_face_features_data, val_Ydata)\n",
    "test_error = compute_error(test_left_eye_data, test_face_features_data, test_Ydata)\n",
    "    \n",
    "print(\"Train Error ==> \", train_error)\n",
    "print(\"Val Error ==> \",  val_error)\n",
    "print(\"Test Error ==> \" ,test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model3 = copy.copy(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(x,y,m,n):\n",
    "    \n",
    "    print(x.shape, y.shape, m.shape, n.shape)\n",
    "    x1 = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3], x.shape[4]))\n",
    "    y1 = y.reshape((y.shape[0]*y.shape[1], y.shape[2], y.shape[3], y.shape[4]))\n",
    "    \n",
    "    m1 = m.reshape((m.shape[0]*m.shape[1], m.shape[2]))\n",
    "    n1 = n.reshape((n.shape[0]*n.shape[1], n.shape[2]))\n",
    "    \n",
    "    return x1,y1,m1,n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def after_calibration(model2, left_eye_data12, right_eye_data12, face_features_data12, Ydata12 ):\n",
    "    train_error = []; val_error = []; test_error = []\n",
    "    \n",
    "    for i in range(len(left_eye_data12)):\n",
    "        #model3 = copy.copy(model31)\n",
    "        \n",
    "        leye, reye, ff, Y = get_shape(left_eye_data12[i], right_eye_data12[i], face_features_data12[i], Ydata12[i])\n",
    "        train_leye, train_reye, train_ff, train_Y = leye[:1000], reye[:1000], ff[:1000], Y[:1000]\n",
    "        test_leye, test_reye, test_ff, test_Y = leye[1000:], reye[1000:], ff[1000:], Y[1000:]\n",
    "        \n",
    "        model2.fit([train_leye, np.concatenate((train_ff[:,1:8],train_ff[:,9:15]), axis =1)],train_Y, \n",
    "                     epochs=20, batch_size=40, verbose=0, shuffle= True)\n",
    "            \n",
    "        #y_pred = model2.predict([test_leye, np.concatenate((test_ff[:,1:8], test_ff[:,8:15]), axis=1).astype(int)])\n",
    "        tr_s = model2.evaluate([train_leye, np.concatenate((train_ff[:,1:8],train_ff[:,9:15]),axis = 1)], train_Y)\n",
    "        t_s = model2.evaluate([test_leye, np.concatenate((test_ff[:,1:8],test_ff[:,9:15]),axis = 1)], test_Y)\n",
    "            \n",
    "        train_error.append(tr_s)\n",
    "        test_error.append(t_s)\n",
    "        \n",
    "        print(tr_s, t_s, \"\\n\")\n",
    "    return np.mean(train_error), np.mean(test_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "# left_eye_data12, right_eye_data12, face_features_data12, Ydata12 = get_data2(users)\n",
    "# print(len(left_eye_data12), len(right_eye_data12), len(face_features_data12),len(Ydata12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, t = after_calibration(model3,left_eye_data12, right_eye_data12, face_features_data12, Ydata12 )\n",
    "print(tr, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(folder, i, model2, n):\n",
    "    \n",
    "    filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "    filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        \n",
    "    video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "    driver_video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "    \n",
    "#     folderX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    \n",
    "#     folderY = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "    leye = filenameX +\"_left_eye_data.npy\"\n",
    "    reye = filenameX +\"_right_eye_data.npy\"\n",
    "    headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "    face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "    if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "       os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "    \n",
    "        x_lefteye = np.load(leye)\n",
    "        x_righteye = np.load(reye)\n",
    "        x_face_features = np.load(headpose_pupil)\n",
    "        x_face_points = np.load(face_points)\n",
    "        y = np.load(filenameY)\n",
    "            \n",
    "        if(y.shape[0] >= n):\n",
    "           # print(i,'y')\n",
    "            arrX_lefteye = x_lefteye[:n]\n",
    "            arrX_righteye = x_righteye[:n]\n",
    "            arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "            \n",
    "            if(i >= 9):\n",
    "                arrY = get_value(y[:n,:])\n",
    "            else:\n",
    "                arrY = y[:n,:]\n",
    "        \n",
    "            # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,8:15]), axis=1).astype(int)])\n",
    "        \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            \n",
    "            plt.figure()\n",
    "            for i in range(int(n/2)):\n",
    "                ret,frame = cap1.read()\n",
    "            #frame  = cv2.resize(frame, (250,250))\n",
    "           # print(folder,i)\n",
    "#             plt.figure()\n",
    "\n",
    "            \n",
    "            plt.imshow(frame)\n",
    "            plt.scatter(arrY[i,0], arrY[i,1], c='g', s=150)\n",
    "            plt.scatter(y_pred[i,0], y_pred[i,1], c='r', s=150)\n",
    "\n",
    "\n",
    "#             plt.imshow(frame)\n",
    "#             plt.scatter(arrY[int(n/2),0], arrY[int(n/2),1], c='g', s=150)\n",
    "#             plt.scatter(y_pred[int(n/2),0], y_pred[int(n/2),1], c='r', s=150)\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data2(folder, i, model2, n):\n",
    "    \n",
    "    filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "    filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        \n",
    "    video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "    driver_video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "    \n",
    "#     folderX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    \n",
    "#     folderY = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "    leye = filenameX +\"_left_eye_data.npy\"\n",
    "    reye = filenameX +\"_right_eye_data.npy\"\n",
    "    headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "    face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "    if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "       os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "    \n",
    "        x_lefteye = np.load(leye)\n",
    "        x_righteye = np.load(reye)\n",
    "        x_face_features = np.load(headpose_pupil)\n",
    "        x_face_points = np.load(face_points)\n",
    "        y = np.load(filenameY)\n",
    "            \n",
    "        if(y.shape[0] >= n):\n",
    "           # print(i,'y')\n",
    "            arrX_lefteye = x_lefteye[:n]\n",
    "            arrX_righteye = x_righteye[:n]\n",
    "            arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "            \n",
    "            if(i >= 9):\n",
    "                arrY = get_value(y[:n,:])\n",
    "            else:\n",
    "                arrY = y[:n,:]\n",
    "        \n",
    "            # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,9:15]), axis=1).astype(int)])\n",
    "        \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "\n",
    "            for i in range(int(n/2)):\n",
    "                ret,frame = cap1.read()\n",
    "                ret1, frame1 = cap2.read()\n",
    "                \n",
    "           # frame_array = np.concatenate((frame1, frame), axis =1)\n",
    "            frame_array = frame1\n",
    "            cv2.circle(frame,(arrY[int(n/2),0], arrY[int(n/2),1]), 70, (0,255,0), -1 )\n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            cv2.circle(frame,(y_pred[int(n/2),0], y_pred[int(n/2),1]), 70, (0,0,255), -1 )\n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            \n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "            ret,frame = cap1.read()\n",
    "            \n",
    "            for j in range(n):\n",
    "                cv2.circle(frame,(arrY[j,0], arrY[j,1]), 70, (0,255,0), -1 )\n",
    "                cv2.circle(frame,(y_pred[j,0], y_pred[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "    \n",
    "                \n",
    "#             plt.figure(figsize=(20,10))\n",
    "#             plt.imshow(frame_array)\n",
    "#             plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "            model2.fit([arrX_lefteye[:1000], np.concatenate((arrX_face_features[:1000,1:8],arrX_face_features[:1000,9:15]), axis =1)],arrY, \n",
    "                     epochs=20, batch_size=40, verbose=0, shuffle= True)\n",
    "            \n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,9:15]), axis=1).astype(int)])\n",
    "        \n",
    "            \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "            ret,frame = cap1.read()\n",
    "            \n",
    "            for j in range(n):\n",
    "                cv2.circle(frame,(arrY[j,0], arrY[j,1]), 70, (0,255,0), -1 )\n",
    "                cv2.circle(frame,(y_pred[j,0], y_pred[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            frame_array = cv2.resize(frame_array, (int(frame_array.shape[1]/4), int(frame_array.shape[0]/4)))\n",
    "            print(frame_array.shape)\n",
    "                \n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(frame_array)\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "            return frame_array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_data3(users, i, model2, n):\n",
    "    \n",
    "#     plt.figure()\n",
    "#     for k in range(len(users)):\n",
    "#         folder = users[k]\n",
    "        \n",
    "#         filenameX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "#         filenameX_face_points = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "#         filenameY= \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "\n",
    "\n",
    "#         video = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "#         driver_video = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "      \n",
    "#         leye = filenameX +\"_left_eye_data.npy\"\n",
    "#         reye = filenameX +\"_right_eye_data.npy\"\n",
    "#         headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "#         face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "#         if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "#            os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "\n",
    "#             x_lefteye = np.load(leye)\n",
    "#             x_righteye = np.load(reye)\n",
    "#             x_face_features = np.load(headpose_pupil)\n",
    "#             x_face_points = np.load(face_points)\n",
    "#             y = np.load(filenameY)\n",
    "\n",
    "#             if(y.shape[0] >= n):\n",
    "#                # print(i,'y')\n",
    "#                 arrX_lefteye = x_lefteye[:n]\n",
    "#                 arrX_righteye = x_righteye[:n]\n",
    "#                 arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "\n",
    "#                 if(i >= 9):\n",
    "#                     arrY = get_value(y[:n,:])\n",
    "#                 else:\n",
    "#                     arrY = y[:n,:]\n",
    "\n",
    "#                 # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "#                 y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,8:15]), axis=1).astype(int)])\n",
    "\n",
    "#               #  print(arrY.shape, y_pred.shape)\n",
    "#                 cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "\n",
    "#                 cv2.circle(frame,(arrY[int(n/2),0], arrY[int(n/2),1]), 150, (0,255,0), -1 )\n",
    "#                 cv2.circle(frame,(y_pred[int(n/2),0], y_pred[int(n/2),1]), 150, (0,0,255), -1 )\n",
    "                \n",
    "#                 frame = cv2.resize(frame, (int(frame.shape[0]/10), int(frame.shape[1]/10)))\n",
    "#                 print(frame.shape)\n",
    "           \n",
    "#        # plt.figure()\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # frame_array = plot_data2(users[0],50,model2,50)\n",
    "# frame_array = plot_data3(users,50,model2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k =0\n",
    "for j in range(10,100,10):\n",
    "    k +=1\n",
    "    image_name = 'qualitative_results'+str(k)+'.png'\n",
    "    for i in range(8):\n",
    "        frame_array = plot_data2(users[i],j+i,model2,50)\n",
    "        if(i ==0):\n",
    "            frame_array2 = frame_array\n",
    "        else:\n",
    "            frame_array2 = np.concatenate((frame_array2, frame_array), axis =0)\n",
    "    cv2.imwrite(image_name, frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('qualitative_results7.png', frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_array2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(10):\n",
    "#     for i in range(112):\n",
    "#       #  plt.figure()\n",
    "#       #  print(users[j],i)\n",
    "#         plot_data(users[j],i,model2,50)\n",
    "#     #break\n",
    "\n",
    "# for j in range(20):\n",
    "#     i = 50\n",
    "#     plot_data(users[j],i,model2,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
