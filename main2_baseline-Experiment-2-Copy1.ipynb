{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation, GRU, Flatten,Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import imutils\n",
    "import dlib\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(data):\n",
    "    arr_row = []\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        pt1 = row[1] + (row[3] - row[1])/2\n",
    "        pt2 = row[2] + (row[4] - row[2])/2\n",
    "        temp =[int(pt1), int(pt2)]\n",
    "        arr_row.append(temp)\n",
    "        \n",
    "    return np.array(arr_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(folder,n):\n",
    "    arr_X_lefteye = []; \n",
    "    arr_X_righteye = []; \n",
    "    arr_X_face_features = []\n",
    "\n",
    "    arr_Y =[]\n",
    "    \n",
    "    for i in range(112):\n",
    "\n",
    "        \n",
    "        filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "        filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "        filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        if(os.path.exists(\"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/\") and os.path.exists(filenameY)\n",
    "          and os.path.exists(filenameX +\"_left_eye_data.npy\") and os.path.exists(filenameX +\"_right_eye_data.npy\")\n",
    "          and os.path.exists(filenameX +\"_headpose_pupil.npy\")):\n",
    "            \n",
    "            x_lefteye = np.load(filenameX +\"_left_eye_data.npy\")\n",
    "            x_righteye = np.load(filenameX +\"_right_eye_data.npy\")\n",
    "            x_face_features = np.load(filenameX +\"_headpose_pupil.npy\")\n",
    "            x_face_points = np.load(filenameX_face_points +\"_face_points.npy\")\n",
    "            #print(i)\n",
    "            y = np.load(filenameY)\n",
    "\n",
    "            if(y.shape[0]>=50):\n",
    "                \n",
    "                arr_X_lefteye.append(x_lefteye[:50])\n",
    "                arr_X_righteye.append(x_righteye[:50])\n",
    "                arr_X_face_features.append(np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1))\n",
    "      \n",
    "                if(i>=9):\n",
    "                    arr_Y.append(get_value(y[:50,:]))\n",
    "                else: \n",
    "                    arr_Y.append(y[:50,:])\n",
    "\n",
    "   # print(np.array(arr_X_lefteye).shape, np.array(arr_X_righteye).shape, np.array(arr_X_face_features).shape, np.array(arr_Y).shape)\n",
    "    return np.array(arr_X_lefteye), np.array(arr_X_righteye), np.array(arr_X_face_features), np.array(arr_Y)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2(users):\n",
    "    left_eye_data = []; right_eye_data = []; face_features_data = []; Ydata = []\n",
    "    \n",
    "    for i in range(len(users)):\n",
    "\n",
    "        left_eye, right_eye, face_features, Yground_truth = get_features(users[i], 50)\n",
    "        print(left_eye.shape, right_eye.shape, face_features.shape, Yground_truth.shape)\n",
    "\n",
    "        left_eye_data.append(left_eye)\n",
    "        right_eye_data.append(right_eye)\n",
    "        face_features_data.append(face_features)\n",
    "        Ydata.append(Yground_truth)\n",
    "\n",
    "    return left_eye_data, right_eye_data, face_features_data, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(users):\n",
    "    for i in range(len(users)):\n",
    "\n",
    "        print(i)\n",
    "        left_eye, right_eye, face_features, Yground_truth = get_features(users[i], 50)\n",
    "\n",
    "        print(left_eye.shape, right_eye.shape, face_features.shape, Yground_truth.shape)\n",
    "\n",
    "        if(i == 0):\n",
    "            left_eye_data = left_eye\n",
    "            right_eye_data = right_eye\n",
    "            face_features_data = face_features\n",
    "            Ydata = Yground_truth\n",
    "\n",
    "        else:\n",
    "            left_eye_data = np.concatenate((left_eye_data, left_eye), axis = 0)\n",
    "            right_eye_data = np.concatenate((right_eye_data, right_eye), axis = 0)\n",
    "            face_features_data = np.concatenate((face_features_data, face_features), axis = 0)\n",
    "            Ydata = np.concatenate((Ydata, Yground_truth), axis = 0)\n",
    "\n",
    "    return left_eye_data, right_eye_data, face_features_data, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "1\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "2\n",
      "(107, 50, 36, 60, 3) (107, 50, 36, 60, 3) (107, 50, 15) (107, 50, 2)\n",
      "3\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "4\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "5\n",
      "(101, 50, 36, 60, 3) (101, 50, 36, 60, 3) (101, 50, 15) (101, 50, 2)\n",
      "6\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "7\n",
      "(99, 50, 36, 60, 3) (99, 50, 36, 60, 3) (99, 50, 15) (99, 50, 2)\n",
      "8\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "9\n",
      "(105, 50, 36, 60, 3) (105, 50, 36, 60, 3) (105, 50, 15) (105, 50, 2)\n",
      "10\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "11\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "12\n",
      "(103, 50, 36, 60, 3) (103, 50, 36, 60, 3) (103, 50, 15) (103, 50, 2)\n",
      "13\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "14\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "15\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "16\n",
      "(100, 50, 36, 60, 3) (100, 50, 36, 60, 3) (100, 50, 15) (100, 50, 2)\n",
      "17\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "18\n",
      "(61, 50, 36, 60, 3) (61, 50, 36, 60, 3) (61, 50, 15) (61, 50, 2)\n",
      "19\n",
      "(57, 50, 36, 60, 3) (57, 50, 36, 60, 3) (57, 50, 15) (57, 50, 2)\n",
      "(2013, 50, 36, 60, 3) (2013, 50, 36, 60, 3) (2013, 50, 15) (2013, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "left_eye_data, right_eye_data, face_features_data,Ydata = get_data(users)\n",
    "print(left_eye_data.shape, right_eye_data.shape, face_features_data.shape,Ydata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert video data into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100650, 36, 60, 3) (100650, 36, 60, 3) (100650, 15) (100650, 2)\n"
     ]
    }
   ],
   "source": [
    "left_eye_data2 = left_eye_data.reshape((left_eye_data.shape[0]*left_eye_data.shape[1], \n",
    "                                         left_eye_data.shape[2],left_eye_data.shape[3],\n",
    "                                         left_eye_data.shape[4]))\n",
    "\n",
    "right_eye_data2 = right_eye_data.reshape((right_eye_data.shape[0]*right_eye_data.shape[1], \n",
    "                                         right_eye_data.shape[2],right_eye_data.shape[3],\n",
    "                                         right_eye_data.shape[4]))\n",
    "\n",
    "face_features_data2 = face_features_data.reshape((face_features_data.shape[0]*face_features_data.shape[1], \n",
    "                                         face_features_data.shape[2]))\n",
    "\n",
    "Ydata2 = Ydata.reshape((Ydata.shape[0]*Ydata.shape[1], \n",
    "                                         Ydata.shape[2]))\n",
    "\n",
    "print(left_eye_data2.shape, right_eye_data2.shape, face_features_data2.shape, Ydata2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into Train and Val\n",
    "\n",
    "1.  Train = 80% of total dataset \n",
    "    - For images\n",
    "       1. left_eye_data2\n",
    "       2. right_eye_data2\n",
    "       3. face_features_data2\n",
    "       4. Ydata2\n",
    "       \n",
    "    - For videos\n",
    "    \n",
    "2.  val = 20% of the dataset\n",
    "3. Test = left_eye10, right_eye10, face_features10, Y10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90585 5032 5033\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "idx = np.arange(0,len(Ydata2))\n",
    "shuffle(idx)\n",
    "\n",
    "train_len = int((90 * len(Ydata2))/100)\n",
    "\n",
    "train_idx = idx[:train_len]\n",
    "#val_idx = idx[train_len:len(Ydata2)]\n",
    "val_idx = idx[train_len:train_len+int((len(Ydata2)-train_len)/2)]\n",
    "test_idx = idx[train_len+int((len(Ydata2)-train_len)/2):len(Ydata2)]\n",
    "\n",
    "print(len(train_idx),len(val_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left_eye_data = left_eye_data2[train_idx]\n",
    "train_right_eye_data = right_eye_data2[train_idx]\n",
    "train_face_features_data = face_features_data2[train_idx] \n",
    "train_Ydata = Ydata2[train_idx]\n",
    "\n",
    "val_left_eye_data = left_eye_data2[val_idx]\n",
    "val_right_eye_data = right_eye_data2[val_idx]\n",
    "val_face_features_data = face_features_data2[val_idx] \n",
    "val_Ydata = Ydata2[val_idx]\n",
    "\n",
    "test_left_eye_data = left_eye_data2[test_idx]\n",
    "test_right_eye_data = right_eye_data2[test_idx]\n",
    "test_face_features_data = face_features_data2[test_idx] \n",
    "test_Ydata = Ydata2[test_idx]\n",
    "\n",
    "\n",
    "# test_left_eye_data = left_eye10.reshape((left_eye10.shape[0]*left_eye10.shape[1], \n",
    "#                                          left_eye10.shape[2],left_eye10.shape[3],\n",
    "#                                          left_eye10.shape[4]))  \n",
    "\n",
    "# test_right_eye_data = right_eye10.reshape((right_eye10.shape[0]*right_eye10.shape[1], \n",
    "#                                          right_eye10.shape[2],right_eye10.shape[3],\n",
    "#                                          right_eye10.shape[4]))  \n",
    "\n",
    "# test_face_features_data = face_features10.reshape((face_features10.shape[0]*face_features10.shape[1], \n",
    "#                                          face_features10.shape[2]))\n",
    "\n",
    "# test_Ydata = Y10.reshape((Y10.shape[0]*Y10.shape[1], \n",
    "#                                          Y10.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90585, 36, 60, 3) (90585, 36, 60, 3) (90585, 15) (90585, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_left_eye_data.shape, train_right_eye_data.shape, train_face_features_data.shape, train_Ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00000000e+00 5.39783621e+00 4.39537096e+00 1.35579329e+01\n",
      " 3.37000000e+02 1.99000000e+02 1.83000000e+02 2.06000000e+02\n",
      " 9.14480000e+04 2.44000000e+02 2.99000000e+02 2.48000000e+02\n",
      " 7.69000000e+02 3.40000000e+01 5.55000000e+02]\n",
      "[  5.39783621   4.39537096  13.55793285 337.         199.\n",
      " 183.         206.         244.         299.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_face_features_data[0])\n",
    "### 0. Frame no\n",
    "### 1,2,3. Headpose(1,2,3)\n",
    "### 4,5 Left eye location\n",
    "### 6,7 Right Eye Location\n",
    "### 8 Face Area\n",
    "### 9,10 Nose location\n",
    "\n",
    "print(np.concatenate((train_face_features_data[0,1:8], train_face_features_data[0,9:11])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 34, 58, 20)        560       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 17, 29, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 27, 50)        9050      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 13, 50)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4550)              0         \n",
      "=================================================================\n",
      "Total params: 9,610\n",
      "Trainable params: 9,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isha.d/.local/lib/python3.5/site-packages/ipykernel_launcher.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(3, 3),activation='relu',input_shape=(36,60,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Conv2D(50, (3, 3), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Conv2D(50, (3, 3), activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "inputs = Input(shape=(10,))\n",
    "#model2= model\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(16, activation ='relu', input_dim=(13)))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Merge([model, model3], mode = 'concat'))\n",
    "\n",
    "model2.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "#model2.add(Dense(500))\n",
    "model2.add(Dense(2))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 48s 529us/step - loss: 162.0072 - val_loss: 148.3911\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 30s 332us/step - loss: 144.5175 - val_loss: 148.0468\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 30s 328us/step - loss: 136.7988 - val_loss: 132.5082\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 30s 332us/step - loss: 130.2822 - val_loss: 126.3210\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 30s 331us/step - loss: 124.2318 - val_loss: 121.5752\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 30s 332us/step - loss: 118.8705 - val_loss: 120.7171\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 30s 330us/step - loss: 114.7833 - val_loss: 116.7675\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 29s 321us/step - loss: 111.0515 - val_loss: 110.4964\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 108.3661 - val_loss: 112.8920\n",
      "Epoch 10/300\n",
      "90585/90585 [==============================] - 29s 319us/step - loss: 105.6571 - val_loss: 108.3012\n",
      "Epoch 11/300\n",
      "90585/90585 [==============================] - 29s 319us/step - loss: 103.6362 - val_loss: 108.1158\n",
      "Epoch 12/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 101.6541 - val_loss: 108.0320\n",
      "Epoch 13/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 100.1621 - val_loss: 107.2952\n",
      "Epoch 14/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 98.7469 - val_loss: 104.1726\n",
      "Epoch 15/300\n",
      "90585/90585 [==============================] - 29s 316us/step - loss: 97.5087 - val_loss: 107.6434\n",
      "Epoch 16/300\n",
      "90585/90585 [==============================] - 29s 316us/step - loss: 96.2165 - val_loss: 103.3614\n",
      "Epoch 17/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 95.0567 - val_loss: 101.4628\n",
      "Epoch 18/300\n",
      "90585/90585 [==============================] - 29s 316us/step - loss: 94.2421 - val_loss: 104.2206\n",
      "Epoch 19/300\n",
      "90585/90585 [==============================] - 29s 315us/step - loss: 93.3216 - val_loss: 104.0685\n",
      "Epoch 20/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 92.5506 - val_loss: 103.2719\n",
      "Epoch 21/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 91.9585 - val_loss: 100.9092\n",
      "Epoch 22/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 90.9746 - val_loss: 100.0742\n",
      "Epoch 23/300\n",
      "90585/90585 [==============================] - 29s 316us/step - loss: 90.5572 - val_loss: 99.2367\n",
      "Epoch 24/300\n",
      "90585/90585 [==============================] - 29s 316us/step - loss: 89.7819 - val_loss: 101.4081\n",
      "Epoch 25/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 89.3316 - val_loss: 98.3336\n",
      "Epoch 26/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 88.8403 - val_loss: 101.6882\n",
      "Epoch 27/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 88.3945 - val_loss: 96.7370\n",
      "Epoch 28/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 87.6632 - val_loss: 98.8848\n",
      "Epoch 29/300\n",
      "90585/90585 [==============================] - 29s 316us/step - loss: 87.0325 - val_loss: 102.0172\n",
      "Epoch 30/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 87.0246 - val_loss: 98.5816\n",
      "Epoch 31/300\n",
      "90585/90585 [==============================] - 29s 318us/step - loss: 86.5164 - val_loss: 98.9180\n",
      "Epoch 32/300\n",
      "90585/90585 [==============================] - 29s 317us/step - loss: 85.9815 - val_loss: 98.5543\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 85.5983 - val_loss: 97.6153\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 85.2293 - val_loss: 95.3080\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 314us/step - loss: 85.0782 - val_loss: 95.8293\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 84.5332 - val_loss: 96.2481\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 84.4181 - val_loss: 96.5567\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 314us/step - loss: 83.8566 - val_loss: 98.1748\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 83.6875 - val_loss: 95.0515\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 83.4219 - val_loss: 95.8328\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 83.4503 - val_loss: 94.5212\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 82.9738 - val_loss: 97.2290\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 82.3162 - val_loss: 97.3397\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 82.1403 - val_loss: 96.6933\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 82.2397 - val_loss: 95.0313\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 81.8204 - val_loss: 94.8057\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 81.6875 - val_loss: 94.0539\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 81.2438 - val_loss: 93.6063\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 81.1457 - val_loss: 94.8102\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 314us/step - loss: 80.8639 - val_loss: 93.6635\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 80.6673 - val_loss: 92.5735\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 80.5641 - val_loss: 93.1084\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 80.5290 - val_loss: 96.4248\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 80.3437 - val_loss: 96.7309\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 80.2120 - val_loss: 94.9646\n",
      "Epoch 10/300\n",
      "90585/90585 [==============================] - 28s 314us/step - loss: 79.8774 - val_loss: 93.3594\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 79.5690 - val_loss: 93.9701\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 79.2977 - val_loss: 94.5296\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 79.4058 - val_loss: 92.5111\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 79.3066 - val_loss: 95.1275\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 79.3078 - val_loss: 92.2752\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 78.9275 - val_loss: 91.8438\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 79.0810 - val_loss: 94.2986\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 78.9828 - val_loss: 93.2393\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 78.6851 - val_loss: 96.4900\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 78.5079 - val_loss: 92.7917\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 78.6601 - val_loss: 93.3076\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 78.2083 - val_loss: 91.0258\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 78.2421 - val_loss: 91.8636\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 78.1921 - val_loss: 92.7458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 77.9047 - val_loss: 93.8565\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 77.9091 - val_loss: 93.3040\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 77.7784 - val_loss: 92.1795\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 77.6383 - val_loss: 92.8900\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 77.5096 - val_loss: 92.9489\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 77.7369 - val_loss: 93.5149\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 77.3014 - val_loss: 93.0145\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 77.3681 - val_loss: 91.2221\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 77.3504 - val_loss: 91.9490\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 76.9620 - val_loss: 93.1572\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 76.8640 - val_loss: 90.6237\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 76.8458 - val_loss: 93.1729\n",
      "Epoch 10/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 76.7954 - val_loss: 93.0598\n",
      "dict_keys(['val_loss', 'loss'])\n",
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 76.7003 - val_loss: 92.3558\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 76.4608 - val_loss: 91.5951\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 76.4791 - val_loss: 91.4418\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 28s 314us/step - loss: 76.4603 - val_loss: 92.1648\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 28s 311us/step - loss: 76.4539 - val_loss: 89.3506\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 76.0477 - val_loss: 90.3907\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 28s 313us/step - loss: 75.9769 - val_loss: 91.4664\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 76.0339 - val_loss: 94.1671\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 28s 312us/step - loss: 75.8912 - val_loss: 92.3641\n",
      "Epoch 10/300\n",
      " 6000/90585 [>.............................] - ETA: 26s - loss: 74.4970"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',min_delta = 1, patience =5, verbose =0, mode ='auto')\n",
    "\n",
    "    history = model2.fit([train_left_eye_data, np.concatenate((train_face_features_data[:,1:8],train_face_features_data[:,9:15]), axis =1)], train_Ydata, \n",
    "                         epochs=300, batch_size=40, callbacks=[earlystopping], validation_data=([val_left_eye_data,\n",
    "                         np.concatenate((val_face_features_data[:,1:8],val_face_features_data[:,9:15]),axis = 1)],val_Ydata), verbose=1, shuffle= True)\n",
    "    print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def compute_error(left_eye_data, face_features_data, Ydata):\n",
    "    y_true = Ydata\n",
    "#     y_pred = model2.predict([left_eye_data, face_features_data[:,1:]]).astype(int)\n",
    "#     error = rmse(y_true, y_pred)\n",
    "#     print(error)\n",
    "#     return error\n",
    "\n",
    "#     # load\n",
    "#     json_file = open('model.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     loaded_model = model_from_json(loaded_model_json)\n",
    "#     # load weights into new model\n",
    "#     loaded_model.load_weights(\"model.h5\")\n",
    "#     print(\"Loaded model from disk\")\n",
    "\n",
    "#     # evaluate loaded model on test data\n",
    "#     loaded_model.compile(loss='mae', optimizer='adam')\n",
    "    scores = model2.evaluate([left_eye_data, np.concatenate((face_features_data[:,1:8],face_features_data[:,9:15]),axis = 1)], Ydata)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = compute_error(train_left_eye_data, train_face_features_data, train_Ydata)\n",
    "val_error = compute_error(val_left_eye_data, val_face_features_data, val_Ydata)\n",
    "test_error = compute_error(test_left_eye_data, test_face_features_data, test_Ydata)\n",
    "    \n",
    "print(\"Train Error ==> \", train_error)\n",
    "print(\"Val Error ==> \",  val_error)\n",
    "print(\"Test Error ==> \" ,test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model3 = copy.copy(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(x,y,m,n):\n",
    "    \n",
    "    print(x.shape, y.shape, m.shape, n.shape)\n",
    "    x1 = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3], x.shape[4]))\n",
    "    y1 = y.reshape((y.shape[0]*y.shape[1], y.shape[2], y.shape[3], y.shape[4]))\n",
    "    \n",
    "    m1 = m.reshape((m.shape[0]*m.shape[1], m.shape[2]))\n",
    "    n1 = n.reshape((n.shape[0]*n.shape[1], n.shape[2]))\n",
    "    \n",
    "    return x1,y1,m1,n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def after_calibration(model2, left_eye_data12, right_eye_data12, face_features_data12, Ydata12 ):\n",
    "    train_error = []; val_error = []; test_error = []\n",
    "    \n",
    "    for i in range(len(left_eye_data12)):\n",
    "        #model3 = copy.copy(model31)\n",
    "        \n",
    "        leye, reye, ff, Y = get_shape(left_eye_data12[i], right_eye_data12[i], face_features_data12[i], Ydata12[i])\n",
    "        train_leye, train_reye, train_ff, train_Y = leye[:1000], reye[:1000], ff[:1000], Y[:1000]\n",
    "        test_leye, test_reye, test_ff, test_Y = leye[1000:], reye[1000:], ff[1000:], Y[1000:]\n",
    "        \n",
    "        model2.fit([train_leye, np.concatenate((train_ff[:,1:8],train_ff[:,9:15]), axis =1)],train_Y, \n",
    "                     epochs=20, batch_size=40, verbose=0, shuffle= True)\n",
    "            \n",
    "        #y_pred = model2.predict([test_leye, np.concatenate((test_ff[:,1:8], test_ff[:,8:15]), axis=1).astype(int)])\n",
    "        tr_s = model2.evaluate([train_leye, np.concatenate((train_ff[:,1:8],train_ff[:,9:15]),axis = 1)], train_Y)\n",
    "        t_s = model2.evaluate([test_leye, np.concatenate((test_ff[:,1:8],test_ff[:,9:15]),axis = 1)], test_Y)\n",
    "            \n",
    "        train_error.append(tr_s)\n",
    "        test_error.append(t_s)\n",
    "        \n",
    "        print(tr_s, t_s, \"\\n\")\n",
    "    return np.mean(train_error), np.mean(test_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "# left_eye_data12, right_eye_data12, face_features_data12, Ydata12 = get_data2(users)\n",
    "# print(len(left_eye_data12), len(right_eye_data12), len(face_features_data12),len(Ydata12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, t = after_calibration(model3,left_eye_data12, right_eye_data12, face_features_data12, Ydata12 )\n",
    "print(tr, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(folder, i, model2, n):\n",
    "    \n",
    "    filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "    filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        \n",
    "    video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "    driver_video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "    \n",
    "#     folderX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    \n",
    "#     folderY = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "    leye = filenameX +\"_left_eye_data.npy\"\n",
    "    reye = filenameX +\"_right_eye_data.npy\"\n",
    "    headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "    face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "    if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "       os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "    \n",
    "        x_lefteye = np.load(leye)\n",
    "        x_righteye = np.load(reye)\n",
    "        x_face_features = np.load(headpose_pupil)\n",
    "        x_face_points = np.load(face_points)\n",
    "        y = np.load(filenameY)\n",
    "            \n",
    "        if(y.shape[0] >= n):\n",
    "           # print(i,'y')\n",
    "            arrX_lefteye = x_lefteye[:n]\n",
    "            arrX_righteye = x_righteye[:n]\n",
    "            arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "            \n",
    "            if(i >= 9):\n",
    "                arrY = get_value(y[:n,:])\n",
    "            else:\n",
    "                arrY = y[:n,:]\n",
    "        \n",
    "            # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,8:15]), axis=1).astype(int)])\n",
    "        \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            \n",
    "            plt.figure()\n",
    "            for i in range(int(n/2)):\n",
    "                ret,frame = cap1.read()\n",
    "            #frame  = cv2.resize(frame, (250,250))\n",
    "           # print(folder,i)\n",
    "#             plt.figure()\n",
    "\n",
    "            \n",
    "            plt.imshow(frame)\n",
    "            plt.scatter(arrY[i,0], arrY[i,1], c='g', s=150)\n",
    "            plt.scatter(y_pred[i,0], y_pred[i,1], c='r', s=150)\n",
    "\n",
    "\n",
    "#             plt.imshow(frame)\n",
    "#             plt.scatter(arrY[int(n/2),0], arrY[int(n/2),1], c='g', s=150)\n",
    "#             plt.scatter(y_pred[int(n/2),0], y_pred[int(n/2),1], c='r', s=150)\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data2(folder, i, model2, n):\n",
    "    \n",
    "    filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "    filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        \n",
    "    video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "    driver_video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "    \n",
    "#     folderX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    \n",
    "#     folderY = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "    leye = filenameX +\"_left_eye_data.npy\"\n",
    "    reye = filenameX +\"_right_eye_data.npy\"\n",
    "    headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "    face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "    if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "       os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "    \n",
    "        x_lefteye = np.load(leye)\n",
    "        x_righteye = np.load(reye)\n",
    "        x_face_features = np.load(headpose_pupil)\n",
    "        x_face_points = np.load(face_points)\n",
    "        y = np.load(filenameY)\n",
    "            \n",
    "        if(y.shape[0] >= n):\n",
    "           # print(i,'y')\n",
    "            arrX_lefteye = x_lefteye[:n]\n",
    "            arrX_righteye = x_righteye[:n]\n",
    "            arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "            \n",
    "            if(i >= 9):\n",
    "                arrY = get_value(y[:n,:])\n",
    "            else:\n",
    "                arrY = y[:n,:]\n",
    "        \n",
    "            # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,9:15]), axis=1).astype(int)])\n",
    "        \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "\n",
    "            for i in range(int(n/2)):\n",
    "                ret,frame = cap1.read()\n",
    "                ret1, frame1 = cap2.read()\n",
    "                \n",
    "           # frame_array = np.concatenate((frame1, frame), axis =1)\n",
    "            frame_array = frame1\n",
    "            cv2.circle(frame,(arrY[int(n/2),0], arrY[int(n/2),1]), 70, (0,255,0), -1 )\n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            cv2.circle(frame,(y_pred[int(n/2),0], y_pred[int(n/2),1]), 70, (0,0,255), -1 )\n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            \n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "            ret,frame = cap1.read()\n",
    "            \n",
    "            for j in range(n):\n",
    "                cv2.circle(frame,(arrY[j,0], arrY[j,1]), 70, (0,255,0), -1 )\n",
    "                cv2.circle(frame,(y_pred[j,0], y_pred[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "    \n",
    "                \n",
    "#             plt.figure(figsize=(20,10))\n",
    "#             plt.imshow(frame_array)\n",
    "#             plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "            model2.fit([arrX_lefteye[:1000], np.concatenate((arrX_face_features[:1000,1:8],arrX_face_features[:1000,9:15]), axis =1)],arrY, \n",
    "                     epochs=20, batch_size=40, verbose=0, shuffle= True)\n",
    "            \n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,9:15]), axis=1).astype(int)])\n",
    "        \n",
    "            \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "            ret,frame = cap1.read()\n",
    "            \n",
    "            for j in range(n):\n",
    "                cv2.circle(frame,(arrY[j,0], arrY[j,1]), 70, (0,255,0), -1 )\n",
    "                cv2.circle(frame,(y_pred[j,0], y_pred[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            frame_array = cv2.resize(frame_array, (int(frame_array.shape[1]/4), int(frame_array.shape[0]/4)))\n",
    "            print(frame_array.shape)\n",
    "                \n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(frame_array)\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "            return frame_array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_data3(users, i, model2, n):\n",
    "    \n",
    "#     plt.figure()\n",
    "#     for k in range(len(users)):\n",
    "#         folder = users[k]\n",
    "        \n",
    "#         filenameX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "#         filenameX_face_points = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "#         filenameY= \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "\n",
    "\n",
    "#         video = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "#         driver_video = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "      \n",
    "#         leye = filenameX +\"_left_eye_data.npy\"\n",
    "#         reye = filenameX +\"_right_eye_data.npy\"\n",
    "#         headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "#         face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "#         if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "#            os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "\n",
    "#             x_lefteye = np.load(leye)\n",
    "#             x_righteye = np.load(reye)\n",
    "#             x_face_features = np.load(headpose_pupil)\n",
    "#             x_face_points = np.load(face_points)\n",
    "#             y = np.load(filenameY)\n",
    "\n",
    "#             if(y.shape[0] >= n):\n",
    "#                # print(i,'y')\n",
    "#                 arrX_lefteye = x_lefteye[:n]\n",
    "#                 arrX_righteye = x_righteye[:n]\n",
    "#                 arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "\n",
    "#                 if(i >= 9):\n",
    "#                     arrY = get_value(y[:n,:])\n",
    "#                 else:\n",
    "#                     arrY = y[:n,:]\n",
    "\n",
    "#                 # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "#                 y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,8:15]), axis=1).astype(int)])\n",
    "\n",
    "#               #  print(arrY.shape, y_pred.shape)\n",
    "#                 cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "\n",
    "#                 cv2.circle(frame,(arrY[int(n/2),0], arrY[int(n/2),1]), 150, (0,255,0), -1 )\n",
    "#                 cv2.circle(frame,(y_pred[int(n/2),0], y_pred[int(n/2),1]), 150, (0,0,255), -1 )\n",
    "                \n",
    "#                 frame = cv2.resize(frame, (int(frame.shape[0]/10), int(frame.shape[1]/10)))\n",
    "#                 print(frame.shape)\n",
    "           \n",
    "#        # plt.figure()\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # frame_array = plot_data2(users[0],50,model2,50)\n",
    "# frame_array = plot_data3(users,50,model2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k =0\n",
    "for j in range(10,100,10):\n",
    "    k +=1\n",
    "    image_name = 'qualitative_results'+str(k)+'.png'\n",
    "    for i in range(8):\n",
    "        frame_array = plot_data2(users[i],j+i,model2,50)\n",
    "        if(i ==0):\n",
    "            frame_array2 = frame_array\n",
    "        else:\n",
    "            frame_array2 = np.concatenate((frame_array2, frame_array), axis =0)\n",
    "    cv2.imwrite(image_name, frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('qualitative_results7.png', frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_array2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(10):\n",
    "#     for i in range(112):\n",
    "#       #  plt.figure()\n",
    "#       #  print(users[j],i)\n",
    "#         plot_data(users[j],i,model2,50)\n",
    "#     #break\n",
    "\n",
    "# for j in range(20):\n",
    "#     i = 50\n",
    "#     plot_data(users[j],i,model2,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
