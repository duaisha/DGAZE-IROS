{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation, GRU, Flatten,Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import imutils\n",
    "import dlib\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(data):\n",
    "    arr_row = []\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        pt1 = row[1] + (row[3] - row[1])/2\n",
    "        pt2 = row[2] + (row[4] - row[2])/2\n",
    "        temp =[int(pt1), int(pt2)]\n",
    "        arr_row.append(temp)\n",
    "        \n",
    "    return np.array(arr_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(folder,n):\n",
    "    arr_X_lefteye = []; \n",
    "    arr_X_righteye = []; \n",
    "    arr_X_face_features = []\n",
    "\n",
    "    arr_Y =[]\n",
    "    \n",
    "    for i in range(112):\n",
    "\n",
    "        \n",
    "        filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "        filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "        filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        if(os.path.exists(\"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/\") and os.path.exists(filenameY)\n",
    "          and os.path.exists(filenameX +\"_left_eye_data.npy\") and os.path.exists(filenameX +\"_right_eye_data.npy\")\n",
    "          and os.path.exists(filenameX +\"_headpose_pupil.npy\")):\n",
    "            \n",
    "            x_lefteye = np.load(filenameX +\"_left_eye_data.npy\")\n",
    "            x_righteye = np.load(filenameX +\"_right_eye_data.npy\")\n",
    "            x_face_features = np.load(filenameX +\"_headpose_pupil.npy\")\n",
    "            x_face_points = np.load(filenameX_face_points +\"_face_points.npy\")\n",
    "            #print(i)\n",
    "            y = np.load(filenameY)\n",
    "\n",
    "            if(y.shape[0]>=50):\n",
    "                \n",
    "                arr_X_lefteye.append(x_lefteye[:50])\n",
    "                arr_X_righteye.append(x_righteye[:50])\n",
    "                arr_X_face_features.append(np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1))\n",
    "      \n",
    "                if(i>=9):\n",
    "                    arr_Y.append(get_value(y[:50,:]))\n",
    "                else: \n",
    "                    arr_Y.append(y[:50,:])\n",
    "\n",
    "   # print(np.array(arr_X_lefteye).shape, np.array(arr_X_righteye).shape, np.array(arr_X_face_features).shape, np.array(arr_Y).shape)\n",
    "    return np.array(arr_X_lefteye), np.array(arr_X_righteye), np.array(arr_X_face_features), np.array(arr_Y)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2(users):\n",
    "    left_eye_data = []; right_eye_data = []; face_features_data = []; Ydata = []\n",
    "    \n",
    "    for i in range(len(users)):\n",
    "\n",
    "        left_eye, right_eye, face_features, Yground_truth = get_features(users[i], 50)\n",
    "        print(left_eye.shape, right_eye.shape, face_features.shape, Yground_truth.shape)\n",
    "\n",
    "        left_eye_data.append(left_eye)\n",
    "        right_eye_data.append(right_eye)\n",
    "        face_features_data.append(face_features)\n",
    "        Ydata.append(Yground_truth)\n",
    "\n",
    "    return left_eye_data, right_eye_data, face_features_data, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(users):\n",
    "    for i in range(len(users)):\n",
    "\n",
    "        print(i)\n",
    "        left_eye, right_eye, face_features, Yground_truth = get_features(users[i], 50)\n",
    "\n",
    "        print(left_eye.shape, right_eye.shape, face_features.shape, Yground_truth.shape)\n",
    "\n",
    "        if(i == 0):\n",
    "            left_eye_data = left_eye\n",
    "            right_eye_data = right_eye\n",
    "            face_features_data = face_features\n",
    "            Ydata = Yground_truth\n",
    "\n",
    "        else:\n",
    "            left_eye_data = np.concatenate((left_eye_data, left_eye), axis = 0)\n",
    "            right_eye_data = np.concatenate((right_eye_data, right_eye), axis = 0)\n",
    "            face_features_data = np.concatenate((face_features_data, face_features), axis = 0)\n",
    "            Ydata = np.concatenate((Ydata, Yground_truth), axis = 0)\n",
    "\n",
    "    return left_eye_data, right_eye_data, face_features_data, Ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "1\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "2\n",
      "(107, 50, 36, 60, 3) (107, 50, 36, 60, 3) (107, 50, 15) (107, 50, 2)\n",
      "3\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "4\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "5\n",
      "(101, 50, 36, 60, 3) (101, 50, 36, 60, 3) (101, 50, 15) (101, 50, 2)\n",
      "6\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "7\n",
      "(99, 50, 36, 60, 3) (99, 50, 36, 60, 3) (99, 50, 15) (99, 50, 2)\n",
      "8\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "9\n",
      "(105, 50, 36, 60, 3) (105, 50, 36, 60, 3) (105, 50, 15) (105, 50, 2)\n",
      "10\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "11\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "12\n",
      "(103, 50, 36, 60, 3) (103, 50, 36, 60, 3) (103, 50, 15) (103, 50, 2)\n",
      "13\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "14\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "15\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "16\n",
      "(100, 50, 36, 60, 3) (100, 50, 36, 60, 3) (100, 50, 15) (100, 50, 2)\n",
      "17\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "18\n",
      "(61, 50, 36, 60, 3) (61, 50, 36, 60, 3) (61, 50, 15) (61, 50, 2)\n",
      "19\n",
      "(57, 50, 36, 60, 3) (57, 50, 36, 60, 3) (57, 50, 15) (57, 50, 2)\n",
      "(2013, 50, 36, 60, 3) (2013, 50, 36, 60, 3) (2013, 50, 15) (2013, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "left_eye_data, right_eye_data, face_features_data,Ydata = get_data(users)\n",
    "print(left_eye_data.shape, right_eye_data.shape, face_features_data.shape,Ydata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert video data into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100650, 36, 60, 3) (100650, 36, 60, 3) (100650, 15) (100650, 2)\n"
     ]
    }
   ],
   "source": [
    "left_eye_data2 = left_eye_data.reshape((left_eye_data.shape[0]*left_eye_data.shape[1], \n",
    "                                         left_eye_data.shape[2],left_eye_data.shape[3],\n",
    "                                         left_eye_data.shape[4]))\n",
    "\n",
    "right_eye_data2 = right_eye_data.reshape((right_eye_data.shape[0]*right_eye_data.shape[1], \n",
    "                                         right_eye_data.shape[2],right_eye_data.shape[3],\n",
    "                                         right_eye_data.shape[4]))\n",
    "\n",
    "face_features_data2 = face_features_data.reshape((face_features_data.shape[0]*face_features_data.shape[1], \n",
    "                                         face_features_data.shape[2]))\n",
    "\n",
    "Ydata2 = Ydata.reshape((Ydata.shape[0]*Ydata.shape[1], \n",
    "                                         Ydata.shape[2]))\n",
    "\n",
    "print(left_eye_data2.shape, right_eye_data2.shape, face_features_data2.shape, Ydata2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into Train and Val\n",
    "\n",
    "1.  Train = 80% of total dataset \n",
    "    - For images\n",
    "       1. left_eye_data2\n",
    "       2. right_eye_data2\n",
    "       3. face_features_data2\n",
    "       4. Ydata2\n",
    "       \n",
    "    - For videos\n",
    "    \n",
    "2.  val = 20% of the dataset\n",
    "3. Test = left_eye10, right_eye10, face_features10, Y10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90585 5032 5033\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "idx = np.arange(0,len(Ydata2))\n",
    "shuffle(idx)\n",
    "\n",
    "train_len = int((90 * len(Ydata2))/100)\n",
    "\n",
    "train_idx = idx[:train_len]\n",
    "#val_idx = idx[train_len:len(Ydata2)]\n",
    "val_idx = idx[train_len:train_len+int((len(Ydata2)-train_len)/2)]\n",
    "test_idx = idx[train_len+int((len(Ydata2)-train_len)/2):len(Ydata2)]\n",
    "\n",
    "print(len(train_idx),len(val_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left_eye_data = left_eye_data2[train_idx]\n",
    "train_right_eye_data = right_eye_data2[train_idx]\n",
    "train_face_features_data = face_features_data2[train_idx] \n",
    "train_Ydata = Ydata2[train_idx]\n",
    "\n",
    "val_left_eye_data = left_eye_data2[val_idx]\n",
    "val_right_eye_data = right_eye_data2[val_idx]\n",
    "val_face_features_data = face_features_data2[val_idx] \n",
    "val_Ydata = Ydata2[val_idx]\n",
    "\n",
    "test_left_eye_data = left_eye_data2[test_idx]\n",
    "test_right_eye_data = right_eye_data2[test_idx]\n",
    "test_face_features_data = face_features_data2[test_idx] \n",
    "test_Ydata = Ydata2[test_idx]\n",
    "\n",
    "\n",
    "# test_left_eye_data = left_eye10.reshape((left_eye10.shape[0]*left_eye10.shape[1], \n",
    "#                                          left_eye10.shape[2],left_eye10.shape[3],\n",
    "#                                          left_eye10.shape[4]))  \n",
    "\n",
    "# test_right_eye_data = right_eye10.reshape((right_eye10.shape[0]*right_eye10.shape[1], \n",
    "#                                          right_eye10.shape[2],right_eye10.shape[3],\n",
    "#                                          right_eye10.shape[4]))  \n",
    "\n",
    "# test_face_features_data = face_features10.reshape((face_features10.shape[0]*face_features10.shape[1], \n",
    "#                                          face_features10.shape[2]))\n",
    "\n",
    "# test_Ydata = Y10.reshape((Y10.shape[0]*Y10.shape[1], \n",
    "#                                          Y10.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90585, 36, 60, 3) (90585, 36, 60, 3) (90585, 15) (90585, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_left_eye_data.shape, train_right_eye_data.shape, train_face_features_data.shape, train_Ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.80000000e+01 -7.09662151e+00  1.20760508e+01  2.45052299e+01\n",
      "  3.35000000e+02  1.92000000e+02  2.08000000e+02  1.71000000e+02\n",
      "  8.47620000e+04  2.36000000e+02  2.66000000e+02  1.77000000e+02\n",
      "  6.98000000e+02  1.77000000e+02  6.98000000e+02]\n",
      "[ -7.09662151  12.07605076  24.50522995 335.         192.\n",
      " 208.         171.         236.         266.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_face_features_data[0])\n",
    "### 0. Frame no\n",
    "### 1,2,3. Headpose(1,2,3)\n",
    "### 4,5 Left eye location\n",
    "### 6,7 Right Eye Location\n",
    "### 8 Face Area\n",
    "### 9,10 Nose location\n",
    "\n",
    "print(np.concatenate((train_face_features_data[0,1:8], train_face_features_data[0,9:11])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 34, 58, 20)        560       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 17, 29, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 15, 27, 50)        9050      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 13, 50)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4550)              0         \n",
      "=================================================================\n",
      "Total params: 9,610\n",
      "Trainable params: 9,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isha.d/.local/lib/python3.5/site-packages/ipykernel_launcher.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(3, 3),activation='relu',input_shape=(36,60,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Conv2D(50, (3, 3), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Conv2D(50, (3, 3), activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "inputs = Input(shape=(10,))\n",
    "#model2= model\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(16, activation ='relu', input_dim=(14)))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Merge([model, model3], mode = 'concat'))\n",
    "\n",
    "model2.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "#model2.add(Dense(500))\n",
    "model2.add(Dense(2))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90585 samples, validate on 5032 samples\n",
      "Epoch 1/300\n",
      "90585/90585 [==============================] - 18s 197us/step - loss: 186.9280 - val_loss: 161.2581\n",
      "Epoch 2/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 153.9943 - val_loss: 153.1963\n",
      "Epoch 3/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 145.9575 - val_loss: 143.1711\n",
      "Epoch 4/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 138.4775 - val_loss: 135.3758\n",
      "Epoch 5/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 132.2088 - val_loss: 128.6102\n",
      "Epoch 6/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 126.4644 - val_loss: 128.0146\n",
      "Epoch 7/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 121.7266 - val_loss: 119.8811\n",
      "Epoch 8/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 117.7837 - val_loss: 117.9496\n",
      "Epoch 9/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 114.3297 - val_loss: 115.1094\n",
      "Epoch 10/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 111.4543 - val_loss: 113.2696\n",
      "Epoch 11/300\n",
      "90585/90585 [==============================] - 15s 164us/step - loss: 108.9380 - val_loss: 118.7466\n",
      "Epoch 12/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 106.8703 - val_loss: 110.0251\n",
      "Epoch 13/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 104.8834 - val_loss: 108.5844\n",
      "Epoch 14/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 103.4436 - val_loss: 105.5172\n",
      "Epoch 15/300\n",
      "90585/90585 [==============================] - 15s 164us/step - loss: 102.1927 - val_loss: 104.8722\n",
      "Epoch 16/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 100.6544 - val_loss: 107.0180\n",
      "Epoch 17/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 100.1401 - val_loss: 106.9679\n",
      "Epoch 18/300\n",
      "90585/90585 [==============================] - 15s 164us/step - loss: 98.9952 - val_loss: 110.1392\n",
      "Epoch 19/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 98.0220 - val_loss: 103.2565\n",
      "Epoch 20/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 97.1549 - val_loss: 103.0840\n",
      "Epoch 21/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 96.7606 - val_loss: 104.8953\n",
      "Epoch 22/300\n",
      "90585/90585 [==============================] - 15s 164us/step - loss: 95.8255 - val_loss: 107.1984\n",
      "Epoch 23/300\n",
      "90585/90585 [==============================] - 15s 163us/step - loss: 95.3107 - val_loss: 103.6168\n",
      "Epoch 24/300\n",
      "90585/90585 [==============================] - 15s 162us/step - loss: 94.5525 - val_loss: 103.1476\n",
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',min_delta = 1, patience =5, verbose =0, mode ='auto')\n",
    "\n",
    "history = model2.fit([train_left_eye_data, np.concatenate((train_face_features_data[:,1:9],train_face_features_data[:,9:15]), axis =1)], train_Ydata, \n",
    "                     epochs=300, batch_size=40, callbacks=[earlystopping], validation_data=([val_left_eye_data,\n",
    "                     np.concatenate((val_face_features_data[:,1:9],val_face_features_data[:,9:15]),axis = 1)],val_Ydata), verbose=1, shuffle= True)\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isha.d/.local/lib/python3.5/site-packages/keras/models.py:332: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/keras/engine/topology.py:1269: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_yaml\n",
    "model2 = load_model('ICRA_trained_fa')\n",
    "model2.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def compute_error(left_eye_data, face_features_data, Ydata):\n",
    "    y_true = Ydata\n",
    "#     y_pred = model2.predict([left_eye_data, face_features_data[:,1:]]).astype(int)\n",
    "#     error = rmse(y_true, y_pred)\n",
    "#     print(error)\n",
    "#     return error\n",
    "\n",
    "#     # load\n",
    "#     json_file = open('model.json', 'r')\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     json_file.close()\n",
    "#     loaded_model = model_from_json(loaded_model_json)\n",
    "#     # load weights into new model\n",
    "#     loaded_model.load_weights(\"model.h5\")\n",
    "#     print(\"Loaded model from disk\")\n",
    "\n",
    "#     # evaluate loaded model on test data\n",
    "#     loaded_model.compile(loss='mae', optimizer='adam')\n",
    "#     model2 = load_model('ICRA_trained')\n",
    "#     model2.compile(loss='mae', optimizer='adam')\n",
    "    scores = model2.evaluate([left_eye_data, np.concatenate((face_features_data[:,1:9],face_features_data[:,9:15]),axis = 1)], Ydata)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90585/90585 [==============================] - 9s 94us/step\n",
      "5032/5032 [==============================] - 0s 69us/step\n",
      "5033/5033 [==============================] - 0s 70us/step\n",
      "Train Error ==>  91.21307988409741\n",
      "Val Error ==>  103.14756058054623\n",
      "Test Error ==>  102.28965198858675\n"
     ]
    }
   ],
   "source": [
    "train_error = compute_error(train_left_eye_data, train_face_features_data, train_Ydata)\n",
    "val_error = compute_error(val_left_eye_data, val_face_features_data, val_Ydata)\n",
    "test_error = compute_error(test_left_eye_data, test_face_features_data, test_Ydata)\n",
    "    \n",
    "print(\"Train Error ==> \", train_error)\n",
    "print(\"Val Error ==> \",  val_error)\n",
    "print(\"Test Error ==> \" ,test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, model):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(model_name + '.yaml', \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_name + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def load_model(model_name):\n",
    "    yaml_file = open(model_name + '.yaml', 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_name + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "save_model('ICRA_trained_fa', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(x,y,m,n):\n",
    "    \n",
    "    print(x.shape, y.shape, m.shape, n.shape)\n",
    "    x1 = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3], x.shape[4]))\n",
    "    y1 = y.reshape((y.shape[0]*y.shape[1], y.shape[2], y.shape[3], y.shape[4]))\n",
    "    \n",
    "    m1 = m.reshape((m.shape[0]*m.shape[1], m.shape[2]))\n",
    "    n1 = n.reshape((n.shape[0]*n.shape[1], n.shape[2]))\n",
    "    \n",
    "    return x1,y1,m1,n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from keras.models import model_from_yaml\n",
    "from keras.optimizers import SGD, Adam, Adamax\n",
    "\n",
    "def after_calibration(model2, left_eye_data12, right_eye_data12, face_features_data12, Ydata12 ):\n",
    "    train_error = []; val_error = []; test_error = []\n",
    "    \n",
    "    for i in range(len(left_eye_data12)):\n",
    "        #model3 = copy.copy(model31)\n",
    "        \n",
    "        leye, reye, ff, Y = get_shape(left_eye_data12[i], right_eye_data12[i], face_features_data12[i], Ydata12[i])\n",
    "        train_leye, train_reye, train_ff, train_Y = leye[:1500], reye[:1500], ff[:1500], Y[:1500]\n",
    "        test_leye, test_reye, test_ff, test_Y = leye[1000:], reye[1000:], ff[1000:], Y[1000:]\n",
    "        \n",
    "        earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',min_delta = 1, patience =5, verbose =0, mode ='auto')\n",
    "        opt = Adam(lr=0.000001)\n",
    "        model2 = load_model('ICRA_trained_fa')\n",
    "        model2.compile(loss='mae', optimizer=opt)\n",
    "        model2.fit([train_leye, np.concatenate((train_ff[:,1:9],train_ff[:,9:15]), axis =1)],train_Y, \n",
    "                     epochs=100, batch_size=40, callbacks=[earlystopping],validation_data=([test_leye,\n",
    "                     np.concatenate((test_ff[:,1:9],test_ff[:,9:15]),axis = 1)],test_Y),verbose=1)\n",
    "            \n",
    "        #y_pred = model2.predict([test_leye, np.concatenate((test_ff[:,1:8], test_ff[:,8:15]), axis=1).astype(int)])\n",
    "        tr_s = model2.evaluate([train_leye, np.concatenate((train_ff[:,1:9],train_ff[:,9:15]),axis = 1)], train_Y)\n",
    "        t_s = model2.evaluate([test_leye, np.concatenate((test_ff[:,1:9],test_ff[:,9:15]),axis = 1)], test_Y)\n",
    "            \n",
    "        train_error.append(tr_s)\n",
    "        test_error.append(t_s)\n",
    "        \n",
    "        print(tr_s, t_s, \"\\n\")\n",
    "    return np.mean(train_error), np.mean(test_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "(107, 50, 36, 60, 3) (107, 50, 36, 60, 3) (107, 50, 15) (107, 50, 2)\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "(101, 50, 36, 60, 3) (101, 50, 36, 60, 3) (101, 50, 15) (101, 50, 2)\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "(99, 50, 36, 60, 3) (99, 50, 36, 60, 3) (99, 50, 15) (99, 50, 2)\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "(105, 50, 36, 60, 3) (105, 50, 36, 60, 3) (105, 50, 15) (105, 50, 2)\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "(103, 50, 36, 60, 3) (103, 50, 36, 60, 3) (103, 50, 15) (103, 50, 2)\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "(100, 50, 36, 60, 3) (100, 50, 36, 60, 3) (100, 50, 15) (100, 50, 2)\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "(61, 50, 36, 60, 3) (61, 50, 36, 60, 3) (61, 50, 15) (61, 50, 2)\n",
      "(57, 50, 36, 60, 3) (57, 50, 36, 60, 3) (57, 50, 15) (57, 50, 2)\n",
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "left_eye_data12, right_eye_data12, face_features_data12, Ydata12 = get_data2(users)\n",
    "print(len(left_eye_data12), len(right_eye_data12), len(face_features_data12),len(Ydata12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isha.d/.local/lib/python3.5/site-packages/keras/models.py:332: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/keras/engine/topology.py:1269: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4400 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 57.1988 - val_loss: 60.0770\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 56.8711 - val_loss: 59.9623\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 330us/step - loss: 56.5387 - val_loss: 59.8741\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 56.2384 - val_loss: 59.8215\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 55.9915 - val_loss: 59.7848\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 330us/step - loss: 55.7690 - val_loss: 59.7585\n",
      "1500/1500 [==============================] - 0s 71us/step\n",
      "4400/4400 [==============================] - 0s 72us/step\n",
      "55.64175937906901 59.758521922718394 \n",
      "\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4400 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 63.4352 - val_loss: 95.2661\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 62.9451 - val_loss: 94.9874\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 332us/step - loss: 62.3607 - val_loss: 94.6857\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 61.8141 - val_loss: 94.4638\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 61.3580 - val_loss: 94.2313\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 333us/step - loss: 60.9614 - val_loss: 94.0178\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 330us/step - loss: 60.6094 - val_loss: 93.8486\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 329us/step - loss: 60.2910 - val_loss: 93.7070\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 1s 333us/step - loss: 59.9953 - val_loss: 93.5555\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 328us/step - loss: 59.7361 - val_loss: 93.4242\n",
      "1500/1500 [==============================] - 0s 73us/step\n",
      "4400/4400 [==============================] - 0s 72us/step\n",
      "59.58905773925781 93.42422087235884 \n",
      "\n",
      "(107, 50, 36, 60, 3) (107, 50, 36, 60, 3) (107, 50, 15) (107, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4350 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 62.3192 - val_loss: 82.4003\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 61.1730 - val_loss: 81.6492\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 59.8247 - val_loss: 80.7318\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 58.5337 - val_loss: 79.9500\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 57.4477 - val_loss: 79.2502\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 56.5410 - val_loss: 78.6983\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 320us/step - loss: 55.8321 - val_loss: 78.2242\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 320us/step - loss: 55.2439 - val_loss: 77.8252\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 54.7306 - val_loss: 77.4968\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 54.3093 - val_loss: 77.2130\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 53.9625 - val_loss: 76.9618\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 53.6596 - val_loss: 76.7576\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 53.3758 - val_loss: 76.5647\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 53.1212 - val_loss: 76.4018\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 52.8790 - val_loss: 76.2530\n",
      "1500/1500 [==============================] - 0s 69us/step\n",
      "4350/4350 [==============================] - 0s 70us/step\n",
      "52.752095102945965 76.25300787563981 \n",
      "\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4300 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 49.7060 - val_loss: 86.5912\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 316us/step - loss: 49.2261 - val_loss: 86.3343\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 48.7280 - val_loss: 86.0649\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 48.2650 - val_loss: 85.8264\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 47.8758 - val_loss: 85.6595\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 47.5502 - val_loss: 85.4978\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 47.2664 - val_loss: 85.3445\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 47.0000 - val_loss: 85.2139\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 46.7488 - val_loss: 85.0869\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 315us/step - loss: 46.5045 - val_loss: 84.9729\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 315us/step - loss: 46.2762 - val_loss: 84.8650\n",
      "1500/1500 [==============================] - 0s 68us/step\n",
      "4300/4300 [==============================] - 0s 68us/step\n",
      "46.14107194010417 84.86501266124637 \n",
      "\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4400 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 67.4951 - val_loss: 98.3120\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 317us/step - loss: 66.7270 - val_loss: 98.1545\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 317us/step - loss: 65.9777 - val_loss: 97.9644\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 320us/step - loss: 65.3116 - val_loss: 97.7496\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 64.7982 - val_loss: 97.5531\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 317us/step - loss: 64.3678 - val_loss: 97.3348\n",
      "1500/1500 [==============================] - 0s 69us/step\n",
      "4400/4400 [==============================] - 0s 69us/step\n",
      "64.15014229329427 97.33478451815519 \n",
      "\n",
      "(101, 50, 36, 60, 3) (101, 50, 36, 60, 3) (101, 50, 15) (101, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4050 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 102.5932 - val_loss: 98.6772\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 101.2400 - val_loss: 98.0728\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 99.7668 - val_loss: 97.3991\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 98.3482 - val_loss: 96.8104\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 96.9841 - val_loss: 96.3147\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 95.7126 - val_loss: 95.8527\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 320us/step - loss: 94.5442 - val_loss: 95.4559\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 93.4346 - val_loss: 95.1116\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 92.4300 - val_loss: 94.8136\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 320us/step - loss: 91.4844 - val_loss: 94.5712\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 90.6271 - val_loss: 94.3471\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 89.8419 - val_loss: 94.1366\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 0s 327us/step - loss: 89.1033 - val_loss: 93.9489\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 88.4303 - val_loss: 93.7758\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 322us/step - loss: 87.8133 - val_loss: 93.6188\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 87.2255 - val_loss: 93.4803\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 86.6947 - val_loss: 93.3482\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 86.1946 - val_loss: 93.2162\n",
      "1500/1500 [==============================] - 0s 74us/step\n",
      "4050/4050 [==============================] - 0s 73us/step\n",
      "85.9341162109375 93.21619791289906 \n",
      "\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4300 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 88.1904 - val_loss: 101.6853\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 87.6527 - val_loss: 101.5482\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 87.0881 - val_loss: 101.4101\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 86.5549 - val_loss: 101.3120\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 86.0817 - val_loss: 101.2264\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 330us/step - loss: 85.6517 - val_loss: 101.1748\n",
      "1500/1500 [==============================] - 0s 73us/step\n",
      "4300/4300 [==============================] - 0s 73us/step\n",
      "85.42825118001302 101.17475013910338 \n",
      "\n",
      "(99, 50, 36, 60, 3) (99, 50, 36, 60, 3) (99, 50, 15) (99, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 3950 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 111.6152 - val_loss: 112.8964\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 317us/step - loss: 110.5658 - val_loss: 112.8048\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 318us/step - loss: 109.3800 - val_loss: 112.7095\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 316us/step - loss: 108.2791 - val_loss: 112.6303\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 318us/step - loss: 107.2582 - val_loss: 112.6031\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 319us/step - loss: 106.3276 - val_loss: 112.6152\n",
      "1500/1500 [==============================] - 0s 69us/step\n",
      "3950/3950 [==============================] - 0s 70us/step\n",
      "105.8566103922526 112.61521707655508 \n",
      "\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4200 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 75.2447 - val_loss: 106.6210\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 73.6300 - val_loss: 106.0640\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 71.7262 - val_loss: 105.4149\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 316us/step - loss: 69.7584 - val_loss: 104.8911\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 67.9915 - val_loss: 104.4896\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 66.4140 - val_loss: 104.1947\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 65.0693 - val_loss: 103.9970\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 315us/step - loss: 63.9348 - val_loss: 103.8392\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 314us/step - loss: 62.9435 - val_loss: 103.7644\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 62.1376 - val_loss: 103.7069\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 315us/step - loss: 61.4491 - val_loss: 103.6852\n",
      "1500/1500 [==============================] - 0s 70us/step\n",
      "4200/4200 [==============================] - 0s 70us/step\n",
      "61.13285913085937 103.6851762898763 \n",
      "\n",
      "(105, 50, 36, 60, 3) (105, 50, 36, 60, 3) (105, 50, 15) (105, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4250 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 65.4228 - val_loss: 110.9683\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 64.4420 - val_loss: 110.1483\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 63.3530 - val_loss: 109.1520\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 62.3101 - val_loss: 108.3036\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 61.4163 - val_loss: 107.6006\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 60.6468 - val_loss: 107.0072\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 59.9771 - val_loss: 106.5130\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 59.4065 - val_loss: 106.1104\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 58.9054 - val_loss: 105.7557\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 58.4501 - val_loss: 105.4485\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 58.0268 - val_loss: 105.1643\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 57.6369 - val_loss: 104.9081\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 57.2597 - val_loss: 104.6574\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 56.9094 - val_loss: 104.4368\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 56.5691 - val_loss: 104.2333\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 56.2436 - val_loss: 104.0420\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 55.9159 - val_loss: 103.8527\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 55.6037 - val_loss: 103.6689\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 55.3043 - val_loss: 103.4892\n",
      "1500/1500 [==============================] - 0s 73us/step\n",
      "4250/4250 [==============================] - 0s 72us/step\n",
      "55.14348197428385 103.48921362663718 \n",
      "\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4400 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 52.9569 - val_loss: 85.3012\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 52.6408 - val_loss: 85.1684\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 52.3486 - val_loss: 85.0259\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 52.0757 - val_loss: 84.9119\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 51.8244 - val_loss: 84.8107\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 322us/step - loss: 51.5900 - val_loss: 84.7168\n",
      "1500/1500 [==============================] - 0s 70us/step\n",
      "4400/4400 [==============================] - 0s 70us/step\n",
      "51.465175791422524 84.71680525346235 \n",
      "\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4400 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 72.5501 - val_loss: 99.3592\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 71.1224 - val_loss: 98.7048\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 69.3444 - val_loss: 97.9072\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 67.5753 - val_loss: 97.2503\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 66.0121 - val_loss: 96.7385\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 64.6520 - val_loss: 96.3016\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 63.4677 - val_loss: 95.9833\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 327us/step - loss: 62.4663 - val_loss: 95.7399\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 323us/step - loss: 61.6118 - val_loss: 95.5568\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 325us/step - loss: 60.8937 - val_loss: 95.4302\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 60.2725 - val_loss: 95.3216\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 59.7317 - val_loss: 95.2946\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 59.2660 - val_loss: 95.2683\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 0s 324us/step - loss: 58.8727 - val_loss: 95.2810\n",
      "1500/1500 [==============================] - 0s 73us/step\n",
      "4400/4400 [==============================] - 0s 73us/step\n",
      "58.68027412923177 95.28095284201882 \n",
      "\n",
      "(103, 50, 36, 60, 3) (103, 50, 36, 60, 3) (103, 50, 15) (103, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4150 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 64.6864 - val_loss: 100.1602\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 317us/step - loss: 64.1135 - val_loss: 100.0347\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 63.5254 - val_loss: 99.9292\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 321us/step - loss: 62.9868 - val_loss: 99.8393\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 316us/step - loss: 62.5155 - val_loss: 99.7507\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 316us/step - loss: 62.0801 - val_loss: 99.6593\n",
      "1500/1500 [==============================] - 0s 71us/step\n",
      "4150/4150 [==============================] - 0s 71us/step\n",
      "61.86702542114258 99.6592508302252 \n",
      "\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4300 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 91.2507 - val_loss: 85.9635\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 1s 341us/step - loss: 89.2361 - val_loss: 85.2513\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 1s 336us/step - loss: 86.9696 - val_loss: 84.4900\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 1s 338us/step - loss: 84.7533 - val_loss: 83.9167\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 1s 336us/step - loss: 82.8621 - val_loss: 83.5331\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 81.2761 - val_loss: 83.3051\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 1s 336us/step - loss: 79.9778 - val_loss: 83.1982\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 78.8394 - val_loss: 83.1671\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 1s 340us/step - loss: 77.8859 - val_loss: 83.1991\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 77.0847 - val_loss: 83.2604\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 1s 338us/step - loss: 76.4095 - val_loss: 83.3142\n",
      "1500/1500 [==============================] - 0s 75us/step\n",
      "4300/4300 [==============================] - 0s 75us/step\n",
      "76.09745660400391 83.3141848187114 \n",
      "\n",
      "(108, 50, 36, 60, 3) (108, 50, 36, 60, 3) (108, 50, 15) (108, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4400 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 62.6917 - val_loss: 77.3956\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 333us/step - loss: 61.0720 - val_loss: 76.9181\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 1s 333us/step - loss: 59.2298 - val_loss: 76.4445\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 332us/step - loss: 57.4485 - val_loss: 76.1023\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 55.9081 - val_loss: 75.9007\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 331us/step - loss: 54.5716 - val_loss: 75.8283\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 1s 334us/step - loss: 53.5201 - val_loss: 75.8573\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 1s 335us/step - loss: 52.6870 - val_loss: 75.9379\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 333us/step - loss: 51.9915 - val_loss: 76.0458\n",
      "1500/1500 [==============================] - 0s 74us/step\n",
      "4400/4400 [==============================] - 0s 74us/step\n",
      "51.688802978515625 76.04579484419389 \n",
      "\n",
      "(104, 50, 36, 60, 3) (104, 50, 36, 60, 3) (104, 50, 15) (104, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4200 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 77.9640 - val_loss: 87.5165\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 333us/step - loss: 76.2542 - val_loss: 87.1366\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 1s 334us/step - loss: 74.4249 - val_loss: 86.7761\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 1s 334us/step - loss: 72.7625 - val_loss: 86.5914\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 1s 337us/step - loss: 71.4535 - val_loss: 86.5404\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 1s 334us/step - loss: 70.4506 - val_loss: 86.5994\n",
      "1500/1500 [==============================] - 0s 72us/step\n",
      "4200/4200 [==============================] - 0s 72us/step\n",
      "70.00570633951823 86.5994034176781 \n",
      "\n",
      "(100, 50, 36, 60, 3) (100, 50, 36, 60, 3) (100, 50, 15) (100, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 98.3570 - val_loss: 103.3083\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 97.7829 - val_loss: 102.9792\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 97.1265 - val_loss: 102.6019\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 312us/step - loss: 96.4764 - val_loss: 102.3140\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 95.8859 - val_loss: 102.0545\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 95.3471 - val_loss: 101.8378\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 316us/step - loss: 94.8880 - val_loss: 101.6794\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 94.4628 - val_loss: 101.5247\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 315us/step - loss: 94.0684 - val_loss: 101.4374\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 313us/step - loss: 93.7199 - val_loss: 101.3299\n",
      "1500/1500 [==============================] - 0s 74us/step\n",
      "4000/4000 [==============================] - 0s 73us/step\n",
      "93.51928916422526 101.32988851928711 \n",
      "\n",
      "(106, 50, 36, 60, 3) (106, 50, 36, 60, 3) (106, 50, 15) (106, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 4300 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 80.6431 - val_loss: 101.0607\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 327us/step - loss: 79.1530 - val_loss: 100.6962\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 327us/step - loss: 77.5990 - val_loss: 100.3409\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 328us/step - loss: 76.1938 - val_loss: 100.0989\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 330us/step - loss: 75.0135 - val_loss: 99.8390\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 328us/step - loss: 74.0244 - val_loss: 99.6727\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 328us/step - loss: 73.1624 - val_loss: 99.5003\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 327us/step - loss: 72.4342 - val_loss: 99.3902\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 328us/step - loss: 71.7923 - val_loss: 99.3001\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 326us/step - loss: 71.2041 - val_loss: 99.2285\n",
      "1500/1500 [==============================] - 0s 75us/step\n",
      "4300/4300 [==============================] - 0s 72us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.91971887207032 99.22852495060411 \n",
      "\n",
      "(61, 50, 36, 60, 3) (61, 50, 36, 60, 3) (61, 50, 15) (61, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 2050 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 110.9616 - val_loss: 148.6831\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 109.5027 - val_loss: 147.6830\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 107.8329 - val_loss: 146.5086\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 247us/step - loss: 106.1777 - val_loss: 145.4559\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 249us/step - loss: 104.7159 - val_loss: 144.5337\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 103.3259 - val_loss: 143.7338\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 102.0775 - val_loss: 143.0512\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 249us/step - loss: 100.9678 - val_loss: 142.4318\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 247us/step - loss: 99.9552 - val_loss: 141.9077\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 247us/step - loss: 99.0725 - val_loss: 141.4592\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 249us/step - loss: 98.2920 - val_loss: 141.1288\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 97.6094 - val_loss: 140.8293\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 0s 247us/step - loss: 96.9984 - val_loss: 140.5867\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 0s 252us/step - loss: 96.4640 - val_loss: 140.3464\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 0s 249us/step - loss: 95.9835 - val_loss: 140.1657\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 0s 247us/step - loss: 95.5674 - val_loss: 140.0252\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 95.1843 - val_loss: 139.9364\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 94.8360 - val_loss: 139.8443\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 0s 248us/step - loss: 94.5490 - val_loss: 139.7530\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 0s 250us/step - loss: 94.2850 - val_loss: 139.6916\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 0s 249us/step - loss: 94.0618 - val_loss: 139.6339\n",
      "1500/1500 [==============================] - 0s 77us/step\n",
      "2050/2050 [==============================] - 0s 77us/step\n",
      "93.93252213541666 139.63390098013528 \n",
      "\n",
      "(57, 50, 36, 60, 3) (57, 50, 36, 60, 3) (57, 50, 15) (57, 50, 2)\n",
      "Loaded model from disk\n",
      "Train on 1500 samples, validate on 1850 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 86.6515 - val_loss: 114.2281\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 85.8120 - val_loss: 113.4460\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 0s 239us/step - loss: 84.9330 - val_loss: 112.7112\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 0s 239us/step - loss: 84.1229 - val_loss: 112.0973\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 0s 242us/step - loss: 83.4281 - val_loss: 111.5486\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 82.7922 - val_loss: 111.0930\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 0s 239us/step - loss: 82.2289 - val_loss: 110.6735\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 0s 239us/step - loss: 81.7364 - val_loss: 110.2835\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 0s 240us/step - loss: 81.2584 - val_loss: 109.9391\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 0s 240us/step - loss: 80.8049 - val_loss: 109.6079\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 80.3771 - val_loss: 109.2661\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 0s 242us/step - loss: 79.9685 - val_loss: 108.9102\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 0s 239us/step - loss: 79.5703 - val_loss: 108.6209\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 79.1913 - val_loss: 108.3345\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 0s 240us/step - loss: 78.8442 - val_loss: 108.0349\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 0s 238us/step - loss: 78.5005 - val_loss: 107.7973\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 0s 240us/step - loss: 78.1782 - val_loss: 107.5159\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 77.8627 - val_loss: 107.3027\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 77.5645 - val_loss: 107.0715\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 0s 240us/step - loss: 77.2784 - val_loss: 106.8445\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 76.9987 - val_loss: 106.6398\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 0s 240us/step - loss: 76.7451 - val_loss: 106.4602\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 76.5051 - val_loss: 106.2588\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 0s 242us/step - loss: 76.2887 - val_loss: 106.1271\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 0s 241us/step - loss: 76.0818 - val_loss: 105.9625\n",
      "1500/1500 [==============================] - 0s 77us/step\n",
      "1850/1850 [==============================] - 0s 76us/step\n",
      "75.97123490397135 105.96249803697741 \n",
      "\n",
      "68.79583258412679 94.87936536942416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tr, t = after_calibration(model2,left_eye_data12, right_eye_data12, face_features_data12, Ydata12 )\n",
    "print(tr, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(folder, i, model2, n):\n",
    "    \n",
    "    filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "    filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        \n",
    "    video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "    driver_video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "    \n",
    "#     folderX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    \n",
    "#     folderY = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "    leye = filenameX +\"_left_eye_data.npy\"\n",
    "    reye = filenameX +\"_right_eye_data.npy\"\n",
    "    headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "    face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "    if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "       os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "    \n",
    "        x_lefteye = np.load(leye)\n",
    "        x_righteye = np.load(reye)\n",
    "        x_face_features = np.load(headpose_pupil)\n",
    "        x_face_points = np.load(face_points)\n",
    "        y = np.load(filenameY)\n",
    "            \n",
    "        if(y.shape[0] >= n):\n",
    "           # print(i,'y')\n",
    "            arrX_lefteye = x_lefteye[:n]\n",
    "            arrX_righteye = x_righteye[:n]\n",
    "            arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "            \n",
    "            if(i >= 9):\n",
    "                arrY = get_value(y[:n,:])\n",
    "            else:\n",
    "                arrY = y[:n,:]\n",
    "        \n",
    "            # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,8:15]), axis=1).astype(int)])\n",
    "        \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            \n",
    "            plt.figure()\n",
    "            for i in range(int(n/2)):\n",
    "                ret,frame = cap1.read()\n",
    "            #frame  = cv2.resize(frame, (250,250))\n",
    "           # print(folder,i)\n",
    "#             plt.figure()\n",
    "\n",
    "            \n",
    "            plt.imshow(frame)\n",
    "            plt.scatter(arrY[i,0], arrY[i,1], c='g', s=150)\n",
    "            plt.scatter(y_pred[i,0], y_pred[i,1], c='r', s=150)\n",
    "\n",
    "\n",
    "#             plt.imshow(frame)\n",
    "#             plt.scatter(arrY[int(n/2),0], arrY[int(n/2),1], c='g', s=150)\n",
    "#             plt.scatter(y_pred[int(n/2),0], y_pred[int(n/2),1], c='r', s=150)\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data2(folder, i, model2, n):\n",
    "    \n",
    "    filenameX = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    filenameX_face_points = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "    filenameY= \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        \n",
    "    video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "    driver_video = \"/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "    \n",
    "#     folderX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "    \n",
    "#     folderY = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "    leye = filenameX +\"_left_eye_data.npy\"\n",
    "    reye = filenameX +\"_right_eye_data.npy\"\n",
    "    headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "    face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "    if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "       os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "    \n",
    "        x_lefteye = np.load(leye)\n",
    "        x_righteye = np.load(reye)\n",
    "        x_face_features = np.load(headpose_pupil)\n",
    "        x_face_points = np.load(face_points)\n",
    "        y = np.load(filenameY)\n",
    "            \n",
    "        if(y.shape[0] >= n):\n",
    "           # print(i,'y')\n",
    "            arrX_lefteye = x_lefteye[:n]\n",
    "            arrX_righteye = x_righteye[:n]\n",
    "            arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "            \n",
    "            if(i >= 9):\n",
    "                arrY = get_value(y[:n,:])\n",
    "            else:\n",
    "                arrY = y[:n,:]\n",
    "        \n",
    "            # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,9:15]), axis=1).astype(int)])\n",
    "        \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "\n",
    "            for i in range(int(n/2)):\n",
    "                ret,frame = cap1.read()\n",
    "                ret1, frame1 = cap2.read()\n",
    "                \n",
    "           # frame_array = np.concatenate((frame1, frame), axis =1)\n",
    "            frame_array = frame1\n",
    "            cv2.circle(frame,(arrY[int(n/2),0], arrY[int(n/2),1]), 70, (0,255,0), -1 )\n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            cv2.circle(frame,(y_pred[int(n/2),0], y_pred[int(n/2),1]), 70, (0,0,255), -1 )\n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            \n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "            ret,frame = cap1.read()\n",
    "            \n",
    "            for j in range(n):\n",
    "                cv2.circle(frame,(arrY[j,0], arrY[j,1]), 70, (0,255,0), -1 )\n",
    "                cv2.circle(frame,(y_pred[j,0], y_pred[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "    \n",
    "                \n",
    "#             plt.figure(figsize=(20,10))\n",
    "#             plt.imshow(frame_array)\n",
    "#             plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "            model2.fit([arrX_lefteye[:1000], np.concatenate((arrX_face_features[:1000,1:8],arrX_face_features[:1000,9:15]), axis =1)],arrY, \n",
    "                     epochs=20, batch_size=40, verbose=0, shuffle= True)\n",
    "            \n",
    "            y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,9:15]), axis=1).astype(int)])\n",
    "        \n",
    "            \n",
    "          #  print(arrY.shape, y_pred.shape)\n",
    "            cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "            cap2 = cv2.VideoCapture(driver_video)###### driver ###################\n",
    "            \n",
    "            ret,frame = cap1.read()\n",
    "            \n",
    "            for j in range(n):\n",
    "                cv2.circle(frame,(arrY[j,0], arrY[j,1]), 70, (0,255,0), -1 )\n",
    "                cv2.circle(frame,(y_pred[j,0], y_pred[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "            frame_array =  np.concatenate((frame_array, frame), axis =1)\n",
    "            frame_array = cv2.resize(frame_array, (int(frame_array.shape[1]/4), int(frame_array.shape[0]/4)))\n",
    "            print(frame_array.shape)\n",
    "                \n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(frame_array)\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "            return frame_array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_data3(users, i, model2, n):\n",
    "    \n",
    "#     plt.figure()\n",
    "#     for k in range(len(users)):\n",
    "#         folder = users[k]\n",
    "        \n",
    "#         filenameX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "#         filenameX_face_points = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "#         filenameY= \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "\n",
    "\n",
    "#         video = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".avi\"\n",
    "#         driver_video = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/driver_view/sample_\"+str(i+1)+\".avi\"\n",
    "      \n",
    "#         leye = filenameX +\"_left_eye_data.npy\"\n",
    "#         reye = filenameX +\"_right_eye_data.npy\"\n",
    "#         headpose_pupil = filenameX +\"_headpose_pupil.npy\"\n",
    "#         face_points = filenameX_face_points +\"_face_points.npy\"\n",
    "\n",
    "#         if(os.path.exists(filenameY) and os.path.exists(leye) and os.path.exists(reye) and \n",
    "#            os.path.exists(headpose_pupil) and os.path.exists(video)):\n",
    "\n",
    "#             x_lefteye = np.load(leye)\n",
    "#             x_righteye = np.load(reye)\n",
    "#             x_face_features = np.load(headpose_pupil)\n",
    "#             x_face_points = np.load(face_points)\n",
    "#             y = np.load(filenameY)\n",
    "\n",
    "#             if(y.shape[0] >= n):\n",
    "#                # print(i,'y')\n",
    "#                 arrX_lefteye = x_lefteye[:n]\n",
    "#                 arrX_righteye = x_righteye[:n]\n",
    "#                 arrX_face_features = np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1)\n",
    "\n",
    "#                 if(i >= 9):\n",
    "#                     arrY = get_value(y[:n,:])\n",
    "#                 else:\n",
    "#                     arrY = y[:n,:]\n",
    "\n",
    "#                 # print(arrX_lefteye.shape, arrX_righteye.shape, arrX_face_features.shape)\n",
    "#                 y_pred = model2.predict([arrX_lefteye, np.concatenate((arrX_face_features[:,1:8], arrX_face_features[:,8:15]), axis=1).astype(int)])\n",
    "\n",
    "#               #  print(arrY.shape, y_pred.shape)\n",
    "#                 cap1 = cv2.VideoCapture(video)###### driver ###################\n",
    "\n",
    "#                 cv2.circle(frame,(arrY[int(n/2),0], arrY[int(n/2),1]), 150, (0,255,0), -1 )\n",
    "#                 cv2.circle(frame,(y_pred[int(n/2),0], y_pred[int(n/2),1]), 150, (0,0,255), -1 )\n",
    "                \n",
    "#                 frame = cv2.resize(frame, (int(frame.shape[0]/10), int(frame.shape[1]/10)))\n",
    "#                 print(frame.shape)\n",
    "           \n",
    "#        # plt.figure()\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # frame_array = plot_data2(users[0],50,model2,50)\n",
    "# frame_array = plot_data3(users,50,model2,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k =0\n",
    "for j in range(10,100,10):\n",
    "    k +=1\n",
    "    image_name = 'qualitative_results'+str(k)+'.png'\n",
    "    for i in range(8):\n",
    "        frame_array = plot_data2(users[i],j+i,model2,50)\n",
    "        if(i ==0):\n",
    "            frame_array2 = frame_array\n",
    "        else:\n",
    "            frame_array2 = np.concatenate((frame_array2, frame_array), axis =0)\n",
    "    cv2.imwrite(image_name, frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('qualitative_results7.png', frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_array2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(10):\n",
    "#     for i in range(112):\n",
    "#       #  plt.figure()\n",
    "#       #  print(users[j],i)\n",
    "#         plot_data(users[j],i,model2,50)\n",
    "#     #break\n",
    "\n",
    "# for j in range(20):\n",
    "#     i = 50\n",
    "#     plot_data(users[j],i,model2,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
