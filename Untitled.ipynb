{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/isha.d/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation, GRU, Flatten,Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import imutils\n",
    "import dlib\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(data):\n",
    "    arr_row = []\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        pt1 = row[1] + (row[3] - row[1])/2\n",
    "        pt2 = row[2] + (row[4] - row[2])/2\n",
    "        temp =[int(pt1), int(pt2)]\n",
    "        arr_row.append(temp)\n",
    "        \n",
    "    return np.array(arr_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(folder):\n",
    "    arr_X_lefteye = []\n",
    "    arr_X_righteye = []\n",
    "    arr_X_face_features = []\n",
    "\n",
    "    arr_Y =[]\n",
    "    \n",
    "    for i in range(112):\n",
    "\n",
    "        filenameX = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/sample\"+str(i+1)\n",
    "        filenameX_face_points = \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_points/sample_\"+str(i+1)\n",
    "        filenameY= \"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/original_road_view/sample_\"+str(i+1)+\".npy\"\n",
    "        \n",
    "        if(os.path.exists(\"/ssd_scratch/cvit/isha2/DGM_final2/dataset_samples_callibrated/user\"+str(folder)+\"/explicit_face_features_game/\") and os.path.exists(filenameY)\n",
    "          and os.path.exists(filenameX +\"_left_eye_data.npy\") and os.path.exists(filenameX +\"_right_eye_data.npy\")\n",
    "          and os.path.exists(filenameX +\"_headpose_pupil.npy\")):\n",
    "            \n",
    "            x_lefteye = np.load(filenameX +\"_left_eye_data.npy\")\n",
    "            x_righteye = np.load(filenameX +\"_right_eye_data.npy\")\n",
    "            x_face_features = np.load(filenameX +\"_headpose_pupil.npy\")\n",
    "            x_face_points = np.load(filenameX_face_points +\"_face_points.npy\")\n",
    "            #print(i)\n",
    "            y = np.load(filenameY)\n",
    "\n",
    "            if(y.shape[0]>=50):\n",
    "                \n",
    "                arr_X_lefteye.append(x_lefteye[:50])\n",
    "                arr_X_righteye.append(x_righteye[:50])\n",
    "                arr_X_face_features.append(np.concatenate((x_face_features[:50], x_face_points[:50]),axis =1))\n",
    "      \n",
    "                if(i>=9):\n",
    "                    arr_Y.append(get_value(y[:50,:]))\n",
    "                else: \n",
    "                    arr_Y.append(y[:50,:])\n",
    "\n",
    "   # print(np.array(arr_X_lefteye).shape, np.array(arr_X_righteye).shape, np.array(arr_X_face_features).shape, np.array(arr_Y).shape)\n",
    "    return np.array(arr_X_lefteye), np.array(arr_X_righteye), np.array(arr_X_face_features), np.array(arr_Y)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "(2121, 50, 36, 60, 3) (2121, 50, 36, 60, 3) (2121, 50, 15) (2121, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "users = [2,3,5,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "\n",
    "for i in range(len(users)):\n",
    "    \n",
    "    print(i)\n",
    "    left_eye, right_eye, face_features, Y = get_features(users[i])\n",
    "    \n",
    "    if(i == 0):\n",
    "        left_eye_data = left_eye\n",
    "        right_eye_data = right_eye\n",
    "        face_features_data = face_features\n",
    "        Ydata = Y\n",
    "        \n",
    "    left_eye_data = np.concatenate((left_eye_data, left_eye), axis = 0)\n",
    "    right_eye_data = np.concatenate((right_eye_data, right_eye), axis = 0)\n",
    "    face_features_data = np.concatenate((face_features_data, face_features), axis = 0)\n",
    "    Ydata = np.concatenate((Ydata, Y), axis = 0)\n",
    "    \n",
    "print(left_eye_data.shape, right_eye_data.shape, face_features_data.shape,Ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2121, 50, 36, 60, 3) (2121, 50, 36, 60, 3) (2121, 50, 15) (2121, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "left_eye_data1 = left_eye_data[:,:,:,:,:]\n",
    "right_eye_data1 = right_eye_data[:,:,:,:,:]\n",
    "face_features_data1 = face_features_data[:,:,:]\n",
    "Ydata1 = Ydata[:,:,:]\n",
    "\n",
    "\n",
    "print(left_eye_data1.shape, right_eye_data1.shape, face_features_data1.shape, Ydata1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106050, 36, 60, 3) (106050, 36, 60, 3) (106050, 15) (106050, 2)\n"
     ]
    }
   ],
   "source": [
    "left_eye_data2 = left_eye_data1.reshape((left_eye_data1.shape[0]*left_eye_data1.shape[1], \n",
    "                                         left_eye_data1.shape[2],left_eye_data1.shape[3],\n",
    "                                         left_eye_data1.shape[4]))\n",
    "\n",
    "right_eye_data2 = right_eye_data1.reshape((right_eye_data1.shape[0]*right_eye_data1.shape[1], \n",
    "                                         right_eye_data1.shape[2],right_eye_data1.shape[3],\n",
    "                                         right_eye_data1.shape[4]))\n",
    "\n",
    "face_features_data2 = face_features_data1.reshape((face_features_data1.shape[0]*face_features_data1.shape[1], \n",
    "                                         face_features_data1.shape[2]))\n",
    "\n",
    "Ydata2 = Ydata1.reshape((Ydata1.shape[0]*Ydata1.shape[1], \n",
    "                                         Ydata1.shape[2]))\n",
    "\n",
    "print(left_eye_data2.shape, right_eye_data2.shape, face_features_data2.shape, Ydata2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into Train and Val\n",
    "\n",
    "Only for first 50 frames \n",
    "\n",
    "1.  Train = 80% of total dataset \n",
    "    - For images\n",
    "       1. left_eye_data2\n",
    "       2. right_eye_data2\n",
    "       3. face_features_data2\n",
    "       4. Ydata2\n",
    "       \n",
    "    - For videos\n",
    "    \n",
    "2.  val = 20% of the dataset\n",
    "3. Test = left_eye10, right_eye10, face_features10, Y10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  6.08081198e+00,  4.94098377e+00, -1.44891679e+00,\n",
       "        3.29000000e+02,  1.89000000e+02,  1.72000000e+02,  2.06000000e+02,\n",
       "        9.08040000e+04,  2.44000000e+02,  2.96000000e+02,  2.48000000e+02,\n",
       "        7.69000000e+02,  3.40000000e+01,  5.55000000e+02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_features_data2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95445 5302 5303\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "idx = np.arange(0,len(Ydata2))\n",
    "shuffle(idx)\n",
    "\n",
    "train_len = int((90 * len(Ydata2))/100)\n",
    "\n",
    "train_idx = idx[:train_len]\n",
    "#val_idx = idx[train_len:len(Ydata2)]\n",
    "val_idx = idx[train_len:train_len+int((len(Ydata2)-train_len)/2)]\n",
    "test_idx = idx[train_len+int((len(Ydata2)-train_len)/2):len(Ydata2)]\n",
    "\n",
    "print(len(train_idx),len(val_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left_eye_data = left_eye_data2[train_idx]\n",
    "train_right_eye_data = right_eye_data2[train_idx]\n",
    "train_face_features_data = face_features_data2[train_idx] \n",
    "train_Ydata = Ydata2[train_idx]\n",
    "\n",
    "val_left_eye_data = left_eye_data2[val_idx]\n",
    "val_right_eye_data = right_eye_data2[val_idx]\n",
    "val_face_features_data = face_features_data2[val_idx] \n",
    "val_Ydata = Ydata2[val_idx]\n",
    "\n",
    "test_left_eye_data = left_eye_data2[test_idx]\n",
    "test_right_eye_data = right_eye_data2[test_idx]\n",
    "test_face_features_data = face_features_data2[test_idx] \n",
    "test_Ydata = Ydata2[test_idx]\n",
    "\n",
    "\n",
    "# test_left_eye_data = left_eye10.reshape((left_eye10.shape[0]*left_eye10.shape[1], \n",
    "#                                          left_eye10.shape[2],left_eye10.shape[3],\n",
    "#                                          left_eye10.shape[4]))  \n",
    "\n",
    "# test_right_eye_data = right_eye10.reshape((right_eye10.shape[0]*right_eye10.shape[1], \n",
    "#                                          right_eye10.shape[2],right_eye10.shape[3],\n",
    "#                                          right_eye10.shape[4]))  \n",
    "\n",
    "# test_face_features_data = face_features10.reshape((face_features10.shape[0]*face_features10.shape[1], \n",
    "#                                          face_features10.shape[2]))\n",
    "\n",
    "# test_Ydata = Y10.reshape((Y10.shape[0]*Y10.shape[1], \n",
    "#                                          Y10.shape[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 34, 58, 20)        560       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 29, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 27, 50)        9050      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 13, 50)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4550)              0         \n",
      "=================================================================\n",
      "Total params: 9,610\n",
      "Trainable params: 9,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isha.d/.local/lib/python3.5/site-packages/ipykernel_launcher.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(3, 3),activation='relu',input_shape=(36,60,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Conv2D(50, (3, 3), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Conv2D(50, (3, 3), activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "inputs = Input(shape=(10,))\n",
    "#model2= model\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(16, activation ='relu', input_dim=(14)))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Merge([model, model3], mode = 'concat'))\n",
    "\n",
    "model2.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "#model2.add(Dense(500))\n",
    "model2.add(Dense(2))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95445 samples, validate on 5302 samples\n",
      "Epoch 1/300\n",
      "95445/95445 [==============================] - 15s 155us/step - loss: 195.1291 - val_loss: 168.3614\n",
      "Epoch 2/300\n",
      "95445/95445 [==============================] - 14s 150us/step - loss: 153.3830 - val_loss: 142.2668\n",
      "Epoch 3/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 142.7306 - val_loss: 139.3704\n",
      "Epoch 4/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 136.5777 - val_loss: 132.4280\n",
      "Epoch 5/300\n",
      "95445/95445 [==============================] - 14s 150us/step - loss: 130.0260 - val_loss: 124.3958\n",
      "Epoch 6/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 124.1194 - val_loss: 120.2996\n",
      "Epoch 7/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 118.9171 - val_loss: 116.6622\n",
      "Epoch 8/300\n",
      "95445/95445 [==============================] - 14s 150us/step - loss: 114.5356 - val_loss: 110.8214\n",
      "Epoch 9/300\n",
      "95445/95445 [==============================] - 14s 150us/step - loss: 111.2596 - val_loss: 113.7524\n",
      "Epoch 10/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 108.2478 - val_loss: 109.2805\n",
      "Epoch 11/300\n",
      "95445/95445 [==============================] - 15s 152us/step - loss: 106.0675 - val_loss: 110.2149\n",
      "Epoch 12/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 104.3174 - val_loss: 106.1475\n",
      "Epoch 13/300\n",
      "95445/95445 [==============================] - 15s 152us/step - loss: 102.6457 - val_loss: 104.5776\n",
      "Epoch 14/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 100.8151 - val_loss: 107.9813\n",
      "Epoch 15/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 99.7938 - val_loss: 101.9555\n",
      "Epoch 16/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 98.4168 - val_loss: 100.8355\n",
      "Epoch 17/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 97.5673 - val_loss: 102.4595\n",
      "Epoch 18/300\n",
      "95445/95445 [==============================] - 14s 152us/step - loss: 96.4149 - val_loss: 101.6370\n",
      "Epoch 19/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 95.6865 - val_loss: 99.6055\n",
      "Epoch 20/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 94.8170 - val_loss: 101.1880\n",
      "Epoch 21/300\n",
      "95445/95445 [==============================] - 15s 152us/step - loss: 94.1469 - val_loss: 100.8771\n",
      "Epoch 22/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 93.5927 - val_loss: 100.0049\n",
      "Epoch 23/300\n",
      "95445/95445 [==============================] - 14s 152us/step - loss: 92.7680 - val_loss: 99.1776\n",
      "Epoch 24/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 92.2956 - val_loss: 97.8545\n",
      "Epoch 25/300\n",
      "95445/95445 [==============================] - 14s 152us/step - loss: 92.1645 - val_loss: 97.9618\n",
      "Epoch 26/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 91.5691 - val_loss: 99.1109\n",
      "Epoch 27/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 90.9487 - val_loss: 97.2611\n",
      "Epoch 28/300\n",
      "95445/95445 [==============================] - 15s 153us/step - loss: 90.3174 - val_loss: 103.2160\n",
      "Epoch 29/300\n",
      "95445/95445 [==============================] - 14s 151us/step - loss: 89.8647 - val_loss: 98.3424\n",
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',min_delta = 1, patience =5, verbose =0, mode ='auto')\n",
    "\n",
    "history = model2.fit([train_left_eye_data, train_face_features_data[:,1:]], train_Ydata, epochs=300, \n",
    "                     batch_size=40, callbacks=[earlystopping], validation_data=([val_left_eye_data,\n",
    "                     val_face_features_data[:,1:]],val_Ydata), verbose=1, shuffle= True)\n",
    "print(history.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def compute_error(left_eye_data, face_features_data, Ydata):\n",
    "    y_true = Ydata\n",
    "#     y_pred = model2.predict([left_eye_data, face_features_data[:,1:]]).astype(int)\n",
    "#     error = rmse(y_true, y_pred)\n",
    "#     print(error)\n",
    "#     return error\n",
    "    scores = model2.evaluate([left_eye_data, face_features_data[:,1:]], Ydata)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95445/95445 [==============================] - 6s 58us/step\n",
      "5302/5302 [==============================] - 0s 56us/step\n",
      "5303/5303 [==============================] - 0s 59us/step\n",
      "Train Error ==>  88.41756595937507\n",
      "Val Error ==>  98.34241694245685\n",
      "Test Error ==>  101.7922598450179\n"
     ]
    }
   ],
   "source": [
    "train_error = compute_error(train_left_eye_data, train_face_features_data, train_Ydata)\n",
    "val_error = compute_error(val_left_eye_data, val_face_features_data, val_Ydata)\n",
    "test_error = compute_error(test_left_eye_data, test_face_features_data, test_Ydata)\n",
    "    \n",
    "print(\"Train Error ==> \", train_error)\n",
    "print(\"Val Error ==> \",  val_error)\n",
    "print(\"Test Error ==> \" ,test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_results(ytest, ypred):\n",
    "    \n",
    "    image = np.zeros((1080, 1920))\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0,len(ypred)):\n",
    "\n",
    "       # print(y[i,0], y[i,1], pred_y[i][0], pred_y[i][1])\n",
    "        cv2.circle(image, (ytest[i,0], ytest[i,1]), 10, (255, 0 ,0),50)\n",
    "        cv2.circle(image, (ypred[i][0], ypred[i][1]), 20, (255, 255, 255), 10)\n",
    "\n",
    "        if(count ==2):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "            count =0\n",
    "            image = np.zeros((1080, 1920))\n",
    "          \n",
    "        count +=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results2(gt, pred, lines=True):\n",
    "    # to get correct scale\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(0,0,c='white')\n",
    "    plt.scatter(1920,1080,c='white')\n",
    "    # to get lines\n",
    "    if lines==True:\n",
    "        for ii in range(gt.shape[0]):\n",
    "            plt.plot([gt[ii,0],pred[ii,0]], [gt[ii,1], pred[ii,1]],c='yellow')\n",
    "    # plot points\n",
    "    \n",
    "    plt.scatter(pred[:,0], pred[:,1],s=15, c='red')\n",
    "    plt.scatter(gt[:,0], gt[:,1],s=40,c='blue')\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(left_eye_data, face_features_data, Ydata4):\n",
    "    \n",
    "    y_pred = model2.predict([left_eye_data, face_features_data[:,1:]]).astype(int)\n",
    "   # print(y_pred, Ydata4)\n",
    "   # print(y_pred[:10], Ydata[:10])\n",
    "    plot_image_results(Ydata4, y_pred )\n",
    "    plot_results2(Ydata4[:10,:], y_pred[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(test_left_eye_data[:1000], test_face_features_data[:1000], test_Ydata[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(ytest, ypred):\n",
    "    \n",
    "    #vid = cv2.VideoCapture(video)\n",
    "    count = 0; i =0\n",
    "  #  ret, frame= vid.read()\n",
    "   # image = frame\n",
    "    image = np.zeros((1080, 1920))\n",
    "    while(True):\n",
    "        cv2.circle(image, (ytest[i,0], ytest[i,1]), 5, (255, 255, 255),50)\n",
    "        cv2.circle(image, (ypred[i][0], ypred[i][1]), 20, (255, 255, 255), 10)\n",
    "        \n",
    "        if(count ==10):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "            cv2.imwrite('faltu.jpg',image)\n",
    "            count =0\n",
    "           # ret, frame = vid.read()\n",
    "            #image = frame\n",
    "            image = np.zeros((1080, 1920))\n",
    "           # break\n",
    "#         else:\n",
    "# #             ret, frame= vid.read()\n",
    "#             image = np.zeros((1080, 1920))\n",
    "        count +=1\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_video(folder, finetune):\n",
    "    \n",
    "    left_eye_data1, right_eye_data1, face_features_data1, Y_data1 = get_features(folder)\n",
    "    \n",
    "    left_eye_data2 = left_eye_data1.reshape((left_eye_data1.shape[0]*left_eye_data1.shape[1], \n",
    "                                         left_eye_data1.shape[2],left_eye_data1.shape[3],\n",
    "                                         left_eye_data1.shape[4]))\n",
    "\n",
    "    right_eye_data2 = right_eye_data1.reshape((right_eye_data1.shape[0]*right_eye_data1.shape[1], \n",
    "                                             right_eye_data1.shape[2],right_eye_data1.shape[3],\n",
    "                                             right_eye_data1.shape[4]))\n",
    "\n",
    "    face_features_data2 = face_features_data1.reshape((face_features_data1.shape[0]*face_features_data1.shape[1], \n",
    "                                             face_features_data1.shape[2]))\n",
    "\n",
    "    Ydata2 = Ydata1.reshape((Ydata1.shape[0]*Ydata1.shape[1], \n",
    "                                             Ydata1.shape[2]))\n",
    "    \n",
    "    \n",
    "    if(finetune == 1):\n",
    "        model2.fit([left_eye_data2[:2000], face_features_data2[:2000,1:]], Ydata2[:2000], epochs=10, \n",
    "                     batch_size=40, callbacks=[earlystopping],\n",
    "                    verbose=2, shuffle= True)\n",
    "\n",
    "    y_pred = model2.predict([left_eye_data2[:2000], face_features_data2[:2000,1:]]).astype(int)\n",
    "#     return plot_results2(Ydata2[1000:1010], y_pred[:10])\n",
    "#     for ii in range(0,y_pred.shape[0],100):\n",
    "#         plot_results2(Ydata2[:2000], y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_video(5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_video(7,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
